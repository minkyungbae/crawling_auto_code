# --------- í”„ë¡œì íŠ¸ì—ì„œ importí•œ ëª©ë¡ ---------------
from .models import YouTubeVideo, YouTubeProduct
from config import settings
# --------- seleniumì—ì„œ importí•œ ëª©ë¡ ---------------
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
# --------- webdriverì—ì„œ importí•œ ëª©ë¡ ---------------
from webdriver_manager.chrome import ChromeDriverManager
from contextlib import contextmanager # ë“œë¼ì´ë²„ ê´€ë¦¬í•˜ëŠ” íƒœê·¸
# --------- ê·¸ ì™¸ í¬ë¡¤ë§ ì½”ë“œë¥¼ ìœ„í•´ importí•œ ëª©ë¡ ---------------
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Union, Dict
from urllib.parse import urlparse
from slugify import slugify
import pandas as pd
import logging, time, re, json, os, urllib.parse


# ----------------------------- â¬‡ï¸ logging ì„¤ì • -----------------------------

logger = logging.getLogger(__name__)  # logger.info(), logger.warning()ë§Œ ì¨ì•¼í•´ìš©

def crawl_youtube():
    logger.info("ìœ íŠœë¸Œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    logger.warning("í…ŒìŠ¤íŠ¸ ê²½ê³  ë©”ì‹œì§€ì…ë‹ˆë‹¤.")
    logger.info("ìœ íŠœë¸Œ í¬ë¡¤ë§ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")


# --------- driver í•œ ë²ˆìœ¼ë¡œ ì •ì˜ ---------------
@contextmanager
def create_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # í¬ë¡¬ ì°½ ë„ìš°ì§€ ì•ŠëŠ” ê¸°ëŠ¥(ì£¼ì„ì²˜ë¦¬í•˜ë©´ ì°½ ë„ì›Œì ¸ìš”)
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    logger.info("ğŸŸ¢ ChromeDriver ì‹¤í–‰")
    try:
        yield driver
    except Exception as e:
        logger.error(f"âŒ WebDriver ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
        raise
    finally:
        driver.quit()
        logger.info("ğŸ›‘ ChromeDriver ì¢…ë£Œ")


# ----------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì±„ë„ì˜ ì˜ìƒ ì „ë¶€ ê°€ì§€ê³  ì˜¤ëŠ” í•¨ìˆ˜ -----------------------------

def get_all_video_ids(driver, channel_url) -> List[str]:
    logger.info(f"ğŸ” ì±„ë„ ì˜ìƒ ID ìˆ˜ì§‘ ì‹œì‘: {channel_url}")

    try:
        driver.get(channel_url + '/videos')
        time.sleep(2)

        video_ids = set()
        last_height = driver.execute_script("return document.documentElement.scrollHeight")
        
        # ìŠ¤í¬ë¡¤ 3ë²ˆ í–ˆëŠ”ë°ë„ ìƒˆë¡œê³ ì¹¨ ì•ˆ ë˜ë©´ ìŠ¤í¬ë¡¤ ë©ˆì¶¤
        scroll_retries = 3 
        retry_count = 0

        while True:
            driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.documentElement.scrollHeight")

            if new_height == last_height:
                retry_count += 1
                if retry_count >= scroll_retries:
                    logger.info("âš ï¸ ë” ì´ìƒ ìŠ¤í¬ë¡¤í•  ì½˜í…ì¸  ì—†ìŒ")
                    break
            else:
                retry_count = 0
            last_height = new_height

        soup = BeautifulSoup(driver.page_source, "html.parser")
        video_elements = soup.select("a#video-title")

        video_ids = []
        for element in video_elements:
            href = element.get("href", "")
            if "v=" in href:
                video_id = href.split("v=")[-1].split("&")[0]
                video_ids.append(video_id)
            else:
                logger.warning(f"âš ï¸ href í˜•ì‹ ì´ìƒ: {href}")

        logger.info(f"âœ… ì´ ì˜ìƒ ID {len(video_ids)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return video_ids
            
    except Exception as e:
        logger.error(f"âŒ get_all_video_ids ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
        return []

# ----------------------------- â¬‡ï¸ elementì˜ text ì¶”ì¶œí•˜ëŠ” ìœ í‹¸ í•¨ìˆ˜ -----------------------------
def safe_get_text(element, default=""):
    try:
        return element.text.strip()
    except Exception:
        return default


# ---------------------- â¬‡ï¸ ì¡°íšŒìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: ì¡°íšŒìˆ˜ 1,234íšŒ -> 1234) ----------------------
def parse_view_count(text: str) -> int:
    try:
        if not text:
            return 0
        cleaned = text.replace("ì¡°íšŒìˆ˜", "").replace("íšŒ", "").replace(",", "").strip()
        return int(cleaned)
    except ValueError:
        logger.warning(f"âš ï¸ ì¡°íšŒìˆ˜ íŒŒì‹± ì‹¤íŒ¨: '{text}', ì´ìœ : {e}")
        return 0


# ----------------------------- â¬‡ï¸ êµ¬ë…ì ìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: 1.2ë§Œëª… -> 12000) -----------------------------
def parse_subscriber_count(text: str) -> int:
    try:
        text = text.replace("ëª…", "").replace(",", "").strip()
        if "ì²œ" in text:
            return int(float(text.replace("ì²œ", "")) * 1_000)
        elif "ë§Œ" in text:
            return int(float(text.replace("ë§Œ", "")) * 10_000)
        return int(text)
    except Exception as e:
        logger.warning(f"âš ï¸ êµ¬ë…ì ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: '{text}', ì´ìœ : {e}")
        return 0


# ---------------------- â¬‡ï¸ ì—…ë¡œë“œ ë‚ ì§œ ë¬¸ìì—´ì„ 'YYYYMMDD' í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ----------------------
def parse_upload_date(text: str) -> Union[str, None]:
    try:
        if match := re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.', text):
            year, month, day = match.groups()
            return f"{year}{int(month):02d}{int(day):02d}"
        elif match := re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', text):
            year, month, day = match.groups()
            return f"{year}{int(month):02d}{int(day):02d}"
    except:
        logger.warning(f"âš ï¸ ë‚ ì§œë¥¼ í˜•ì‹ ë³€í™˜ ëª»í–ˆëŠ”ë…??")
    return None


# ---------------------- â¬‡ï¸ ì œí’ˆ ê°œìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: 5ê°œ ì œí’ˆ) ----------------------
def parse_product_count(text: str) -> Union[int, None]:
    try:
        if match := re.search(r'(\d+)\s*ê°œ\s*ì œí’ˆ', text):
            return int(match.group(1))
    except:
        logger.warning(f"âš ï¸ ì œí’ˆ ê°œìˆ˜ ëª» ì°¾ì•˜ëŠ”ë…??")
    return None

# ---------------------- â¬‡ï¸ ë”ë³´ê¸° í´ë¦­ ë° ë”ë³´ê¸°ë€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ----------------------
def click_description(driver) -> str:
    try:
        # ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¼ìœ¼ë¡œì¨ ë²„íŠ¼ì´ ë¡œë“œë˜ë„ë¡ ìœ ë„
        body = driver.find_element(By.TAG_NAME, 'body')
        for _ in range(3):
            body.send_keys(Keys.END)
            time.sleep(1)
        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„ (2ê°€ì§€ selector)
        try:
            expand_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "#expand"))
            )
            driver.execute_script("arguments[0].click();", expand_button)
            logger.info("ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
        except Exception:
            logger.info("ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ ë˜ëŠ” í´ë¦­ ì‹¤íŒ¨, ë¬´ì‹œí•˜ê³  ì§„í–‰")

        selectors = [
            "#description-text-container",
            "#description-inline-expander"
        ]
        for selector in selectors:
            try:
                elem = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                desc = elem.text.strip()
                if desc:
                    return desc
            except Exception:
                logger.debug(f"'{selector}'ë¡œ ì„¤ëª…ë€ ì¶”ì¶œ ì‹¤íŒ¨, ë‹¤ìŒ ì‹œë„")
        logger.warning("ë”ë³´ê¸°ë€ì— ì„¤ëª…ì´ ì—†ìŒ")
        return "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
        
    except Exception as e:
        logger.error(f"âŒ ì„¤ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)
        return "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
    

# ---------------------- â¬‡ï¸ ì˜ìƒ ê¸°ë³¸ ì •ë³´: ì œëª©, ì±„ë„ëª…, êµ¬ë…ì ìˆ˜, ì¡°íšŒìˆ˜, ì—…ë¡œë“œì¼, ì œí’ˆ ê°œìˆ˜ ----------------------
def base_youtube_info(driver, video_url: str) -> pd.DataFrame:
    logger.info("Crawling video: %s", video_url)
    today_str = datetime.today().strftime('%Y%m%d')

    try:
        driver.get(video_url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "description")))
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        video_id = video_url.split("v=")[-1]
        title = safe_get_text(soup.select_one("h1.title")) or "ì œëª© ìˆ˜ì§‘ ì‹¤íŒ¨"
        channel_name = safe_get_text(soup.select_one("#channel-name")) or "ì±„ë„ëª… ìˆ˜ì§‘ ì‹¤íŒ¨"
        subscriber_text = safe_get_text(soup.select_one("#owner-sub-count"))
        subscriber_count = parse_subscriber_count(subscriber_text) if subscriber_text else "êµ¬ë…ì ìˆ˜ ìˆ˜ì§‘ ì‹¤íŒ¨"
        view_text = safe_get_text(soup.select_one(".view-count"))
        view_count = parse_view_count(view_text)
        upload_date = safe_get_text(soup.select_one("#info-strings yt-formatted-string"))
        description = safe_get_text(soup.select_one("#description"))

        # ê¸°ë³¸ ë°ì´í„° ì„¸íŠ¸
        base_data = {
            "video_id": video_id,
            "title": title,
            "channel_name": channel_name,
            "subscriber_count": subscriber_count,
            "view_count": view_count,
            "upload_date": upload_date,
            "extracted_date": today_str,
            "video_url": video_url,
            "description": description
        }
        # ì œí’ˆ ì¶”ì¶œ
        products = extract_products_from_dom(soup)
        product_count = len(products)

        if products > 0:
            # ì œí’ˆ ê°ê°ì— ë©”íƒ€ë°ì´í„° ë³‘í•©
            full_data = [{**base_data, 
                        "product_name": p.get("title", ""), 
                        "product_link": p.get("url", ""),
                        "product_price": None,
                        "product_image_link": None,
                        "product_count": product_count} for p in products]

        else:
            full_data = [{**base_data,
                        "product_name": "ì œí’ˆ ì—†ìŒ",
                        "product_link": None,
                        "product_price": None,
                        "product_image_link": None,
                        "product_count": 0}]
            
        logger.info(f"âœ… ì˜ìƒ ì •ë³´ ë° ì œí’ˆ {product_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return pd.DataFrame(full_data)
    
    except Exception as e:
        logger.error(f"âŒ base_youtube_info ì˜ˆì™¸: {e}", exc_info=True)
        return pd.DataFrame()

#--------------------------------------- ì œí’ˆ ì •ë³´ ì¶”ì¶œ -------------------------------------
def extract_products_from_dom(soup) -> List[Dict]:
    script_tag = soup.find("script", string=lambda s: s and "var ytInitialData" in s)
    if not script_tag:
        logger.warning("âš ï¸ ytInitialData ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ì—†ìŒ")
        return []
    
    try:
        json_text = script_tag.string.split("var ytInitialData =")[-1].rsplit("};", 1)[0] + "}"
        data = json.loads(json_text)
    except json.JSONDecodeError:
        logger.warning("âš ï¸ ytInitialData JSON íŒŒì‹± ì‹¤íŒ¨")
        return []
    
    products = []
    try:
        contents = data["contents"]["twoColumnWatchNextResults"]["results"]["results"]["contents"]
        for item in contents:
            section = item.get("itemSectionRenderer", {}).get("contents", [{}])[0]
            sponsor = section.get("videoDescriptionSponsorSectionRenderer")
            if sponsor:
                for prod in sponsor.get("sponsorSection", {}).get("products", []):
                    title = prod.get("title", {}).get("runs", [{}])[0].get("text", "")
                    url = prod.get("navigationEndpoint", {}).get("commandMetadata", {}).get("webCommandMetadata", {}).get("url", "")
                    if title and url:
                        products.append({"title": title, "url": url})
        return products
    except Exception as e:
        logger.warning(f"âš ï¸ ì œí’ˆ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []


# ----------------------------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì˜ìƒ URL ì ‘ì† í›„ ë°ì´í„° ìˆ˜ì§‘ ìˆ˜í–‰ -----------------------------------------------

def collect_video_data(driver, video_id: str, index: int = None, total: int = None) -> pd.DataFrame:
    base_url = f"https://www.youtube.com/watch?v={video_id}"
    
    try:
        driver.get(base_url)
        if index is not None and total is not None:
            logger.info(f"\nğŸ“¹ ({index}/{total}) í¬ë¡¤ë§ ì¤‘: {video_id}")

        df = base_youtube_info(driver, base_url)

        logger.info(f"ğŸ“¦ ìˆ˜ì§‘ëœ ì œí’ˆ ê°œìˆ˜: {len(df)}")
        if df.empty:
            logger.warning(f"âš ï¸ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŒ: {video_id}")

        return df
    
    except Exception as e:
            logger.error(f"âŒ ì˜ˆì™¸ ë°œìƒ - collect_video_data(): {video_id} | ì—ëŸ¬: {e}")
            return None


# -------------------------------------------- â¬‡ï¸ DB(sqlite) ì €ì¥ í•¨ìˆ˜ ---------------------------------------- 
def save_youtube_data_to_db(df: pd.DataFrame) -> int:
    if df.empty:
        logger.warning("âŒ ì €ì¥í•  ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return 0
    
    # ì˜ìƒì€ video_id ê¸°ì¤€ìœ¼ë¡œ 1ê°œ, ì œí’ˆì€ ì—¬ëŸ¬ ê°œ
    video_data = df.iloc[0]

    try:

        video, created = YouTubeVideo.objects.get_or_create(
            video_id=video_data["video_id"],
            defaults={
                "extracted_date": video_data["extracted_date"],
                "video_url": video_data["video_url"],
                "title": video_data["title"],
                "channel_name": video_data["channel_name"],
                "subscriber_count": video_data["subscriber_count"],
                "view_count": video_data["view_count"],
                "upload_date": video_data["upload_date"],
                "description": video_data["description"]
            }
        )
        if not created:
            logger.info("âš ï¸ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì˜ìƒ idì…ë‹ˆë‹¤: %s", video.video_id)
            return 0

        for _, row in df.iterrows():
            YouTubeProduct.objects.get_or_create(
                video=video,
                url=row["product_link"],
                defaults={
                    "title": row["product_name"],
                    "price": row.get("product_price"),
                    "image_url": row.get("product_image_link")
                }
            )

        logger.info("ğŸ“¹ ì˜ìƒ ë° ì œí’ˆ ì €ì¥ ì™„ë£Œ: %s", video.video_id)
        return len(df)
    
    except Exception as e:
        logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)
        return 0
    
# ------------------------------------- â¬‡ï¸ ì—‘ì…€/CSV ì €ì¥ í•¨ìˆ˜ ------------------------------
def export_videos_to_csv(file_path="youtube_videos.csv"):
    videos = YouTubeVideo.objects.all().prefetch_related("product_set")
    rows = []
    for video in videos:
        for product in video.product_set.all():
            rows.append({
                "video_id": video.video_id,
                "title": video.title,
                "channel_name": video.channel_name,
                "view_count": video.view_count,
                "upload_date": video.upload_date,
                "product_title": product.title,
                "product_url": product.url,
                "product_price": product.price,
                "product_image_link": product.image_url
            })
    df = pd.DataFrame(rows)
    df.to_csv(file_path, index=False, encoding="utf-8-sig")
    logger.info("Exported video and product data to CSV: %s", file_path)


# ------------------------------------- â¬‡ï¸ í¬ë¡¤ë§ëœ ìœ íŠœë¸Œ ì˜ìƒì„ ì¡°íšŒí•˜ê³  ìˆ˜ì •í•˜ëŠ” ì½”ë“œ ------------------------------
def update_youtube_data_to_db(dataframe: pd.DataFrame) -> int:
    if dataframe.empty:
        return 0

    video_id = dataframe.iloc[0]['video_id']
    
    try:
        video = YouTubeVideo.objects.get(video_id=video_id)
        row = dataframe.iloc[0]

        # ê¸°ì¡´ ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸
        video.extracted_date = row['extracted_date']
        video.upload_date = row['upload_date']
        video.channel_name = row['channel_name']
        video.subscriber_count = row['subscriber_count']
        video.video_url = row['video_url']
        video.title = row['title']
        video.view_count = row['view_count']
        video.product_count = row['product_count']
        video.description = row['description']
        video.save()

        # ê¸°ì¡´ ì œí’ˆ ì •ë³´ ì‚­ì œ í›„ ìƒˆë¡œ ì €ì¥
        video.products.all().delete()

        # pd.DataFrame == dataframe
        for _, row in dataframe.iterrows():
            product_name = row.get('product_name')
            if product_name and pd.notna(product_name):
                YouTubeProduct.objects.create(
                    video=video,
                    product_image_link=row.get('product_image_link'),
                    product_name=product_name,
                    product_price=row.get('product_price'),
                    product_link=row.get('product_link'),
                )
        logger.info(f"ğŸ” ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {video_id}")
        return 1

    except YouTubeVideo.DoesNotExist:
        logger.warning(f"âŒ í•´ë‹¹ video_idì— ëŒ€í•œ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤: {video_id}")
        return 0

# ------------------------------------- â¬‡ï¸ ì±„ë„ URLì—ì„œ ê³ ìœ í•œ ID ì¶”ì¶œ (ì˜ˆ: UCxxxx ë˜ëŠ” @handle í˜•ì‹) ------------------------------
def get_channel_id_from_url(channel_url):
    parsed = urlparse(channel_url)
    parts = parsed.path.strip("/").split("/")
    return parts[-1] if parts else "unknown_channel"

# ------------------------------------- â¬‡ï¸ ì±„ë„ URLì—ì„œ ê³ ìœ í•œ ID ì¶”ì¶œ (ì˜ˆ: UCxxxx ë˜ëŠ” @handle í˜•ì‹) ------------------------------
def get_channel_name(driver, channel_url):
    """
    ì±„ë„ ì´ë¦„ì„ YouTube ì±„ë„ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜´.
    """
    driver.get(channel_url)
    driver.implicitly_wait(5)
    try:
        title_element = driver.find_element("xpath", '//meta[@property="og:title"]')
        channel_name = title_element.get_attribute("content")
        return slugify(channel_name)  # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ slugify ì²˜ë¦¬
    except Exception as e:
        logger.warning(f"âš ï¸ ì±„ë„ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return "unknown_channel"
    
# ------------------------------------- â¬‡ï¸ ì—‘ì…€ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def save_to_excel(df: pd.DataFrame, file_path: str):
    try:
        today_str = datetime.now().strftime("%Y%m%d")
        if file_path.lower().endswith(".xlsx"):
            file_path = file_path[:-5] + f"_{today_str}.xlsx"
        else:
            file_path = file_path + f"_{today_str}.xlsx"

        df.to_excel(file_path, index=False)
        logger.info(f"ğŸ’¾ ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {file_path}")
    except Exception as e:
        logger.error(f"âŒ ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)

# ------------------------------------- â¬‡ï¸ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def save_to_db(df: pd.DataFrame):
    from django.db import transaction

    if df.empty:
        logger.warning("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    with transaction.atomic():
        video_cache = {}  # video_id ë³„ë¡œ video ê°ì²´ ìºì‹±
        
        for _, row in df.iterrows():
            video_id = row["video_id"]

            if video_id not in video_cache:
                video_obj, created = YouTubeVideo.objects.update_or_create(
                    video_id=video_id,
                    defaults={
                        "extracted_date": row["extracted_date"],
                        "upload_date": row["upload_date"],
                        "channel_name": row["channel_name"],
                        "subscriber_count": row["subscriber_count"],
                        "title": row["title"],
                        "view_count": row["view_count"],
                        "video_url": row["video_url"],
                        "product_count": row.get("product_count", ""),
                        "description": row["description"],
                    }
                )
                video_cache[video_id] = video_obj
                if created:
                    logger.info(f"DB ì €ì¥ ì™„ë£Œ: {video_obj.video_id}")
                else:
                    logger.info(f"DB ì—…ë°ì´íŠ¸ ì™„ë£Œ: {video_obj.video_id}")
            else:
                video_obj = video_cache[video_id]

            # ì œí’ˆ ì €ì¥
            product_name = row.get("product_name")
            if product_name:
                YouTubeProduct.objects.update_or_create(
                    video=video_obj,
                    product_name=product_name,
                    defaults={
                        "product_price": row.get("product_price", ""),
                        "product_image_link": row.get("product_image_link", ""),
                        "product_link": row.get("product_link", ""),
                    }
                )

# ------------------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì±„ë„ì˜ ì „ì²´ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def crawl_channel_videos(channel_url: str, save_path: str):
    with create_driver() as driver:
        video_ids = get_all_video_ids(driver, channel_url)
        logger.info(f"ì´ {len(video_ids)}ê°œ ì˜ìƒ í¬ë¡¤ë§ ì‹œì‘")
        all_data = pd.DataFrame()

        for idx, vid in enumerate(video_ids, 1):
            video_url = f"https://www.youtube.com/watch?v={vid}"
            df = base_youtube_info(driver, video_url)
            if not df.empty:
                all_data = pd.concat([all_data, df], ignore_index=True)
            else:
                logger.warning(f"âš ï¸ ì˜ìƒ í¬ë¡¤ë§ ì‹¤íŒ¨: {video_url}")
            logger.info(f"ì§„í–‰: {idx}/{len(video_ids)}")

        if not all_data.empty:
            save_to_db(all_data)
            save_to_excel(all_data, save_path)
        else:
            logger.warning("âš ï¸ í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„° ì—†ìŒ")


# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":

    logging.basicConfig(level=logging.INFO)

    channel_urls = [
        "https://www.youtube.com/@yegulyegul8256",
        "https://www.youtube.com/@%EC%B9%A1%EC%B4%89",
    ]
    
    export_dir = "exports"
    if not os.path.exists(export_dir):
        os.makedirs(export_dir)

    for channel_url in channel_urls:
        try:
            logger.info(f"ğŸš€ ì±„ë„ í¬ë¡¤ë§ ì‹œì‘: {channel_url}")

            today_str = datetime.datetime.now().strftime("%Y%m%d")
            channel_name = urllib.parse.unquote(channel_url.split("/")[-1])
            save_path = os.path.join(export_dir,f"{channel_name}_{today_str}.xlsx")

            crawl_channel_videos(channel_url, save_path)
            logger.info(f"âœ… ì±„ë„ í¬ë¡¤ë§ ì™„ë£Œ: {channel_url}")

        except Exception as e:
            logger.warning(f"âŒ ì±„ë„ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {channel_url} - {e}")
        
        time.sleep(3)  # ğŸ”½ ê° ì±„ë„ ê°„ 3ì´ˆ ì‰¬ì—ˆë‹¤ê°€ ë‹¤ìŒ ì±„ë„ ì‹¤í–‰
