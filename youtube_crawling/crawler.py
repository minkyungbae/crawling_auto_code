# --------- í”„ë¡œì íŠ¸ì—ì„œ importí•œ ëª©ë¡ ---------------
from .models import YouTubeVideo, YouTubeProduct
from config import settings
# --------- seleniumì—ì„œ importí•œ ëª©ë¡ ---------------
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
# --------- webdriverì—ì„œ importí•œ ëª©ë¡ ---------------
from webdriver_manager.chrome import ChromeDriverManager
from contextlib import contextmanager # ë“œë¼ì´ë²„ ê´€ë¦¬í•˜ëŠ” íƒœê·¸
# --------- ê·¸ ì™¸ í¬ë¡¤ë§ ì½”ë“œë¥¼ ìœ„í•´ importí•œ ëª©ë¡ ---------------
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Union, Dict
from urllib.parse import urlparse
from slugify import slugify
import pandas as pd
import logging, time, re, json, os


# ----------------------------- â¬‡ï¸ logging ì„¤ì • -----------------------------

logger = logging.getLogger(__name__)  # logger.info(), logger.warning()ë§Œ ì¨ì•¼í•´ìš©

def crawl_youtube():
    logger.info("ìœ íŠœë¸Œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    logger.warning("í…ŒìŠ¤íŠ¸ ê²½ê³  ë©”ì‹œì§€ì…ë‹ˆë‹¤.")
    logger.info("ìœ íŠœë¸Œ í¬ë¡¤ë§ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")


# --------- driver í•œ ë²ˆìœ¼ë¡œ ì •ì˜ ---------------
@contextmanager
def create_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # í¬ë¡¬ ì°½ ë„ìš°ì§€ ì•ŠëŠ” ê¸°ëŠ¥(ì£¼ì„ì²˜ë¦¬í•˜ë©´ ì°½ ë„ì›Œì ¸ìš”)
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    try:
        yield driver
    finally:
        driver.quit()


# ----------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì±„ë„ì˜ ì˜ìƒ ì „ë¶€ ê°€ì§€ê³  ì˜¤ëŠ” í•¨ìˆ˜ -----------------------------

def get_all_video_ids(driver, channel_url):
    logger.info("Fetching all video IDs from channel: %s", channel_url)
    driver.get(channel_url + '/videos')
    time.sleep(2)

    video_ids = set()
    last_height = driver.execute_script("return document.documentElement.scrollHeight")
    
    while True:
        driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);")
        time.sleep(2)
        new_height = driver.execute_script("return document.documentElement.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

    soup = BeautifulSoup(driver.page_source, "html.parser")

    video_elements = soup.select("a#video-title")
    video_ids = [element["href"].split("v=")[-1] for element in video_elements if "href" in element.attrs]
    logger.info("Found %d videos", len(video_ids))
    return video_ids

# ----------------------------- â¬‡ï¸ elementì˜ text ì¶”ì¶œí•˜ëŠ” ìœ í‹¸ í•¨ìˆ˜ -----------------------------
def safe_get_text(element, default=""):
    try:
        return element.text.strip()
    except Exception:
        return default


# ---------------------- â¬‡ï¸ ì¡°íšŒìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: ì¡°íšŒìˆ˜ 1,234íšŒ -> 1234) ----------------------
def parse_view_count(text: str) -> int:
    try:
        return int(text.replace("ì¡°íšŒìˆ˜", "").replace("íšŒ", "").replace(",", "").strip())
    except:
        return 0


# ----------------------------- â¬‡ï¸ êµ¬ë…ì ìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: 1.2ë§Œëª… -> 12000) -----------------------------
def parse_subscriber_count(text: str) -> int:
    try:
        text = text.replace("ëª…", "").replace(",", "").strip()
        if "ì²œ" in text:
            return int(float(text.replace("ì²œ", "")) * 1_000)
        elif "ë§Œ" in text:
            return int(float(text.replace("ë§Œ", "")) * 10_000)
        return int(text)
    except:
        return 0


# ---------------------- â¬‡ï¸ ì—…ë¡œë“œ ë‚ ì§œ ë¬¸ìì—´ì„ 'YYYYMMDD' í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ----------------------
def parse_upload_date(text: str) -> Union[str, None]:
    try:
        if match := re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.', text):
            year, month, day = match.groups()
            return f"{year}{int(month):02d}{int(day):02d}"
        elif match := re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', text):
            year, month, day = match.groups()
            return f"{year}{int(month):02d}{int(day):02d}"
    except:
        logger.warning(f"âš ï¸ ë‚ ì§œë¥¼ í˜•ì‹ ë³€í™˜ ëª»í–ˆëŠ”ë…??")
    return None


# ---------------------- â¬‡ï¸ ì œí’ˆ ê°œìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: 5ê°œ ì œí’ˆ) ----------------------
def parse_product_count(text: str) -> Union[int, None]:
    try:
        if match := re.search(r'(\d+)\s*ê°œ\s*ì œí’ˆ', text):
            return int(match.group(1))
    except:
        logger.warning(f"âš ï¸ ì œí’ˆ ê°œìˆ˜ ëª» ì°¾ì•˜ëŠ”ë…??")
    return None

# ---------------------- â¬‡ï¸ ë”ë³´ê¸° í´ë¦­ ë° ë”ë³´ê¸°ë€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ----------------------
def click_description(driver) -> str:
    try:
        # ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¼ìœ¼ë¡œì¨ ë²„íŠ¼ì´ ë¡œë“œë˜ë„ë¡ ìœ ë„
        body = driver.find_element(By.TAG_NAME, 'body')
        for _ in range(3):
            body.send_keys(Keys.END)
            time.sleep(1)
        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„ (2ê°€ì§€ selector)
        try:
            expand_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.XPATH, "//tp-yt-paper-button[@id='expand']"))
            )
            driver.execute_script("arguments[0].click();", expand_button)
        except:
            pass

        selectors = [
            "#description-inline-expander yt-attributed-string",
            "yt-formatted-string#description"
        ]
        for selector in selectors:
            try:
                elem = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                return elem.text.strip() or "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
            except:
                continue
    except Exception as e:
        logger.error(f"âŒ ì„¤ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
    

# ---------------------- â¬‡ï¸ ì˜ìƒ ê¸°ë³¸ ì •ë³´: ì œëª©, ì±„ë„ëª…, êµ¬ë…ì ìˆ˜, ì¡°íšŒìˆ˜, ì—…ë¡œë“œì¼, ì œí’ˆ ê°œìˆ˜ ----------------------
def base_youtube_info(driver, video_url: str) -> pd.DataFrame:
    logger.info("Crawling video: %s", video_url)
    today_str = datetime.today().strftime('%Y%m%d')
    driver.get(video_url)

    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "description")))
    except Exception:
        logger.warning("âš ï¸ ì„¤ëª…ë€ ë¡œë”© ì‹¤íŒ¨: %s", video_url)
        return pd.DataFrame()

    soup = BeautifulSoup(driver.page_source, "html.parser")

    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    video_id = video_url.split("v=")[-1]
    title = safe_get_text(soup.select_one("h1.title")) or "ì œëª© ìˆ˜ì§‘ ì‹¤íŒ¨"
    channel_name = safe_get_text(soup.select_one("#channel-name")) or "ì±„ë„ëª… ìˆ˜ì§‘ ì‹¤íŒ¨"
    subscriber_text = safe_get_text(soup.select_one("#owner-sub-count"))
    subscriber_count = parse_subscriber_count(subscriber_text) if subscriber_text else "êµ¬ë…ì ìˆ˜ ìˆ˜ì§‘ ì‹¤íŒ¨"
    view_text = safe_get_text(soup.select_one(".view-count"))
    view_count = parse_view_count(view_text)
    upload_date = safe_get_text(soup.select_one("#info-strings yt-formatted-string"))
    description = safe_get_text(soup.select_one("#description"))

    # ê¸°ë³¸ ë°ì´í„° ì„¸íŠ¸
    base_data = {
        "video_id": video_id,
        "title": title,
        "channel_name": channel_name,
        "subscriber_count": subscriber_count,
        "view_count": view_count,
        "upload_date": upload_date,
        "extracted_date": today_str,
        "video_url": video_url,
        "description": description
    }
    try:
        # ì œí’ˆ ì¶”ì¶œ
        product_list = extract_products_from_dom(soup)
        product_count = len(product_list)

        if product_list:
            # ì œí’ˆ ê°ê°ì— ë©”íƒ€ë°ì´í„° ë³‘í•©
            full_data = [{**base_data, **product, "product_count": product_count} for product in product_list]

        else:
            full_data = [{
                **base_data,
                "product_name": "í•´ë‹¹ ì˜ìƒì— í¬í•¨ëœ ì œí’ˆ ì—†ìŒ",
                "product_price": None,
                "product_image_link": None,
                "product_link": None,
                "product_count": 0
            }]
        return pd.DataFrame(full_data)
    
    except Exception as e:
        logger.error("âŒ ì˜ˆì™¸ ë°œìƒ (%s): %s", video_id, e)
        return pd.DataFrame([{
            **base_data,
            "product_name": "í•´ë‹¹ ì˜ìƒì— í¬í•¨ëœ ì œí’ˆ ì—†ìŒ",
            "product_price": None,
            "product_image_link": None,
            "product_link": None,
            "product_count": 0
        }])
    

#--------------------------------------- ì œí’ˆ ì •ë³´ ì¶”ì¶œ -------------------------------------
def extract_products_from_dom(soup):
    script_tag = soup.find("script", string=lambda s: s and "var ytInitialData" in s)
    if not script_tag:
        logger.warning("âš ï¸ ytInitialData script íƒœê·¸ ëª» ì°¾ìŒ")
        return []
    
    json_text = script_tag.string.split("var ytInitialData =")[-1].rsplit("};", 1)[0] + "}"

    try:
        data = json.loads(json_text)
    except json.JSONDecodeError:
        logger.warning("Failed to parse ytInitialData JSON")
        return []
    
    products = []
    try:
        product_items = data["contents"]["twoColumnWatchNextResults"]["results"]["results"]["contents"]
        for item in product_items:
            product_section = item.get("itemSectionRenderer", {}).get("contents", [{}])[0]
            if "videoDescriptionSponsorSectionRenderer" in product_section:
                sponsored = product_section["videoDescriptionSponsorSectionRenderer"]
                for prod in sponsored.get("sponsorSection", {}).get("products", []):
                    title = prod.get("title", {}).get("runs", [{}])[0].get("text", "")
                    url = prod.get("navigationEndpoint", {}).get("urlEndpoint", {}).get("url", "")
                    if url and not any(p["url"] == url for p in products):  # ì¤‘ë³µ ì œê±°
                        products.append({"title": title, "url": url})
    except Exception as e:
        logger.error("Error while extracting products: %s", str(e))
    return products


# ----------------------------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì˜ìƒ URL ì ‘ì† í›„ ë°ì´í„° ìˆ˜ì§‘ ìˆ˜í–‰ -----------------------------------------------

def collect_video_data(driver, video_id: str, index: int = None, total: int = None) -> pd.DataFrame:
    base_url = f"https://www.youtube.com/watch?v={video_id}"
    
    try:
        driver.get(base_url)
        if index is not None and total is not None:
            logger.info(f"\nğŸ“¹ ({index}/{total}) í¬ë¡¤ë§ ì¤‘: {video_id}")

        df = base_youtube_info(driver, base_url)

        logger.info(f"ğŸ“¦ ìˆ˜ì§‘ëœ ì œí’ˆ ê°œìˆ˜: {len(df)}")
        if df.empty:
            logger.warning(f"âš ï¸ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŒ: {video_id}")

        return df
    
    except Exception as e:
            logger.error(f"âŒ ì˜ˆì™¸ ë°œìƒ - collect_video_data(): {video_id} | ì—ëŸ¬: {e}")
            return None


# -------------------------------------------- â¬‡ï¸ DB(sqlite) ì €ì¥ í•¨ìˆ˜ ---------------------------------------- 
def save_youtube_data_to_db(df: pd.DataFrame):
    if df.empty:
        logger.warning("âŒ ì €ì¥í•  ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return
    
    # ì˜ìƒì€ video_id ê¸°ì¤€ìœ¼ë¡œ 1ê°œ, ì œí’ˆì€ ì—¬ëŸ¬ ê°œ
    video_data = df.iloc[0]

    video, created = YouTubeVideo.objects.get_or_create(
        video_id=video_data["video_id"],
        defaults={
            "extracted_date": video_data["extracted_date"],
            "video_url": video_data["video_url"],
            "title": video_data["title"],
            "channel_name": video_data["channel_name"],
            "subscriber_count": video_data["subscriber_count"],
            "view_count": video_data["view_count"],
            "upload_date": video_data["upload_date"],
            "description": video_data["description"]
        }
    )
    if not created:
        logger.info("âš ï¸ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì˜ìƒ idì…ë‹ˆë‹¤: %s", video.video_id)
        return

    for _, row in df.iterrows():
        YouTubeProduct.objects.get_or_create(
            video=video,
            url=row["product_link"],
            defaults={
                "title": row["product_name"],
                "price": row.get("product_price"),
                "image_url": row.get("product_image_link")
            }
        )

    logger.info("ğŸ“¹ ì˜ìƒ ë° ì œí’ˆ ì €ì¥ ì™„ë£Œ: %s", video.video_id)
# ------------------------------------- â¬‡ï¸ ì—‘ì…€/CSV ì €ì¥ í•¨ìˆ˜ ------------------------------
def export_videos_to_csv(file_path="youtube_videos.csv"):
    videos = YouTubeVideo.objects.all().prefetch_related("product_set")
    rows = []
    for video in videos:
        for product in video.product_set.all():
            rows.append({
                "video_id": video.video_id,
                "title": video.title,
                "channel_name": video.channel_name,
                "view_count": video.view_count,
                "upload_date": video.upload_date,
                "product_title": product.title,
                "product_url": product.url,
                "product_price": product.price,
                "product_image_link": product.image_url
            })
    df = pd.DataFrame(rows)
    df.to_csv(file_path, index=False, encoding="utf-8-sig")
    logger.info("Exported video and product data to CSV: %s", file_path)


# ------------------------------------- â¬‡ï¸ í¬ë¡¤ë§ëœ ìœ íŠœë¸Œ ì˜ìƒì„ ì¡°íšŒí•˜ê³  ìˆ˜ì •í•˜ëŠ” ì½”ë“œ ------------------------------
def update_youtube_data_to_db(dataframe: pd.DataFrame) -> int:
    if dataframe.empty:
        return 0

    video_id = dataframe.iloc[0]['video_id']
    
    try:
        video = YouTubeVideo.objects.get(video_id=video_id)
        row = dataframe.iloc[0]

        # ê¸°ì¡´ ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸
        video.extracted_date = row['extracted_date']
        video.upload_date = row['upload_date']
        video.channel_name = row['channel_name']
        video.subscriber_count = row['subscriber_count']
        video.video_url = row['video_url']
        video.title = row['title']
        video.view_count = row['view_count']
        video.product_count = row['product_count']
        video.description = row['description']
        video.save()

        # ê¸°ì¡´ ì œí’ˆ ì •ë³´ ì‚­ì œ í›„ ìƒˆë¡œ ì €ì¥
        video.products.all().delete()

        # pd.DataFrame == dataframe
        for _, row in dataframe.iterrows():
            product_name = row.get('product_name')
            if product_name and pd.notna(product_name):
                YouTubeProduct.objects.create(
                    video=video,
                    product_image_link=row.get('product_image_link'),
                    product_name=product_name,
                    product_price=row.get('product_price'),
                    product_link=row.get('product_link'),
                )
        logger.info(f"ğŸ” ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {video_id}")
        return 1

    except YouTubeVideo.DoesNotExist:
        logger.warning(f"âŒ í•´ë‹¹ video_idì— ëŒ€í•œ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤: {video_id}")
        return 0

# ------------------------------------- â¬‡ï¸ ì±„ë„ URLì—ì„œ ê³ ìœ í•œ ID ì¶”ì¶œ (ì˜ˆ: UCxxxx ë˜ëŠ” @handle í˜•ì‹) ------------------------------
def get_channel_id_from_url(channel_url):
    parsed = urlparse(channel_url)
    parts = parsed.path.strip("/").split("/")
    return parts[-1] if parts else "unknown_channel"

# ------------------------------------- â¬‡ï¸ ì±„ë„ URLì—ì„œ ê³ ìœ í•œ ID ì¶”ì¶œ (ì˜ˆ: UCxxxx ë˜ëŠ” @handle í˜•ì‹) ------------------------------
def get_channel_name(driver, channel_url):
    """
    ì±„ë„ ì´ë¦„ì„ YouTube ì±„ë„ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜´.
    """
    driver.get(channel_url)
    driver.implicitly_wait(5)
    try:
        title_element = driver.find_element("xpath", '//meta[@property="og:title"]')
        channel_name = title_element.get_attribute("content")
        return slugify(channel_name)  # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ slugify ì²˜ë¦¬
    except Exception as e:
        logger.warning(f"âš ï¸ ì±„ë„ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return "unknown_channel"

# ------------------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì±„ë„ì˜ ì „ì²´ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def crawl_channel_videos(channel_urls, export_dir="exports"):
    today_str = datetime.now().strftime("%Y%m%d")
    os.makedirs(export_dir, exist_ok=True)

    with create_driver() as driver:
        for channel_url in channel_urls:
            channel_name = get_channel_name(driver, channel_url)
            logger.info(f"ğŸ“º ì±„ë„ í¬ë¡¤ë§ ì‹œì‘: {channel_name}")

            all_channel_data = []

            video_ids = get_all_video_ids(driver, channel_url)
            for index, video_id in enumerate(video_ids, start=1):
                df = collect_video_data(driver, video_id, index, len(video_ids))
                if df is not None and not df.empty:
                    saved_count = save_youtube_data_to_db(df)
                    all_channel_data.append(df)
                    logger.info(f"âœ… ì €ì¥ëœ ì œí’ˆ ìˆ˜: {saved_count}")
                else:
                    logger.warning(f"ğŸš« ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ ì €ì¥í•˜ì§€ ì•ŠìŒ: {video_id}")

            if all_channel_data:
                combined_df = pd.concat(all_channel_data, ignore_index=True)

                # ì €ì¥ íŒŒì¼ëª… êµ¬ì„±
                filename_base = f"{channel_name}_{today_str}"

                # í´ë” ê²½ë¡œ: ì±„ë„ëª… ê¸°ì¤€ìœ¼ë¡œ í´ë” ìƒì„±
                export_dir = os.path.join(export_dir, channel_name)
                os.makedirs(export_dir, exist_ok=True)

                # íŒŒì¼ ì €ì¥
                csv_path = os.path.join(export_dir, f"{filename_base}.csv")
                excel_path = os.path.join(export_dir, f"{filename_base}.xlsx")

                combined_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
                combined_df.to_excel(excel_path, index=False)

                logger.info(f"ğŸ“ CSV ì €ì¥ ì™„ë£Œ: {csv_path}")
                logger.info(f"ğŸ“ Excel ì €ì¥ ì™„ë£Œ: {excel_path}")
            else:
                logger.warning(f"ğŸš« ì±„ë„ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•ŠìŒ: {channel_name}")

            logger.info(f"ğŸ ì±„ë„ í¬ë¡¤ë§ ì™„ë£Œ: {channel_name}")

    logger.info("ğŸ‰ ëª¨ë“  ì±„ë„ í¬ë¡¤ë§ ë° íŒŒì¼ ì €ì¥ ì™„ë£Œ!")


# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":

    logging.basicConfig(level=logging.INFO)

    channel_urls = [
        "https://www.youtube.com/@yegulyegul8256",
        "https://www.youtube.com/@%EC%B9%A1%EC%B4%89",
    ]

    for channel_url in channel_urls:
        try:
            logger.info(f"ğŸš€ ì±„ë„ í¬ë¡¤ë§ ì‹œì‘: {channel_url}")
            crawl_channel_videos(channel_url)
            logger.info(f"âœ… ì±„ë„ í¬ë¡¤ë§ ì™„ë£Œ: {channel_url}")
        except Exception as e:
            logger.warning(f"âŒ ì±„ë„ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {channel_url} - {e}")
        
        time.sleep(3)  # ğŸ”½ ê° ì±„ë„ ê°„ 3ì´ˆ ì‰¬ì—ˆë‹¤ê°€ ë‹¤ìŒ ì±„ë„ ì‹¤í–‰
