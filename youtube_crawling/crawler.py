# --------- í”„ë¡œì íŠ¸ì—ì„œ importí•œ ëª©ë¡ ---------------
from youtube_crawling.models import YouTubeVideo, YouTubeProduct
# --------- seleniumì—ì„œ importí•œ ëª©ë¡ ---------------
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
# --------- webdriverì—ì„œ importí•œ ëª©ë¡ ---------------
from webdriver_manager.chrome import ChromeDriverManager
from contextlib import contextmanager # ë“œë¼ì´ë²„ ê´€ë¦¬í•˜ëŠ” íƒœê·¸
# --------- ê·¸ ì™¸ í¬ë¡¤ë§ ì½”ë“œë¥¼ ìœ„í•´ importí•œ ëª©ë¡ ---------------
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Union, Dict, Optional
from urllib.parse import urlparse, unquote, parse_qsl
from slugify import slugify
import pandas as pd
import logging, time, re, json, os, urllib.parse


# ----------------------------- â¬‡ï¸ logging ì„¤ì • -----------------------------

logger = logging.getLogger(__name__)  # logger.info(), logger.warning()ë§Œ ì¨ì•¼í•´ìš©

# --------- driver í•œ ë²ˆìœ¼ë¡œ ì •ì˜ ---------------
@contextmanager
def create_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--no-sandbox")           # ìƒŒë“œë°•ìŠ¤ ë¹„í™œì„±í™” (ë³´ì•ˆ ê¸°ëŠ¥ í•´ì œ)
    options.add_argument("--disable-dev-shm-usage")# ê³µìœ  ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„í™œì„±í™”
    options.add_argument("--disable-gpu")          # GPU í•˜ë“œì›¨ì–´ ê°€ì† ë¹„í™œì„±í™”
    options.add_argument("--disable-extensions")   # í¬ë¡¬ í™•ì¥ í”„ë¡œê·¸ë¨ ë¹„í™œì„±í™”
    options.add_argument("--disable-infobars")     # ì •ë³´ í‘œì‹œì¤„ ë¹„í™œì„±í™”
    options.add_argument("--start-maximized")      # ë¸Œë¼ìš°ì € ìµœëŒ€í™”
    options.add_argument("--disable-notifications")# ì•Œë¦¼ ë¹„í™œì„±í™”
    options.add_argument('--ignore-certificate-errors')  # ì¸ì¦ì„œ ì˜¤ë¥˜ ë¬´ì‹œ
    options.add_argument('--ignore-ssl-errors')    # SSL ì˜¤ë¥˜ ë¬´ì‹œ
    # User-Agent ì„¤ì •
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36')
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    logger.info("ğŸŸ¢ ChromeDriver ì‹¤í–‰")
    try:
        yield driver
    except Exception as e:
        logger.error(f"âŒ WebDriver ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
        raise
    finally:
        driver.quit()
        logger.info("ğŸ›‘ ChromeDriver ì¢…ë£Œ")


# ---------------------- â¬‡ï¸ URL ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€ ----------------------
def clean_youtube_url(url: str) -> str:
    """YouTube URLì„ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # URLì—ì„œ 'watch?v=' ë¶€ë¶„ì´ ì¤‘ë³µë˜ëŠ”ì§€ í™•ì¸
        if url.count('watch?v=') > 1:
            # ë§ˆì§€ë§‰ 'watch?v=' ì´í›„ì˜ ë¶€ë¶„ë§Œ ê°€ì ¸ì˜´
            video_id = url.split('watch?v=')[-1]
            return f'https://www.youtube.com/watch?v={video_id}'
        return url
    except Exception as e:
        logger.error(f"âŒ URL ì •ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return url


# ----------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì±„ë„ì˜ ì˜ìƒ ì „ë¶€ ê°€ì§€ê³  ì˜¤ëŠ” í•¨ìˆ˜ -----------------------------
def get_all_video_ids(driver, channel_url):
    logger.info(f"ğŸ” ì±„ë„ ì˜ìƒ ID ìˆ˜ì§‘ ì‹œì‘: {channel_url}")

    try:
        videos_url = channel_url.rstrip('/') + "/videos"
        driver.get(videos_url)
        time.sleep(3)  # í˜ì´ì§€ ë¡œë”©ì„ ìœ„í•œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€

        video_urls = set()
        last_height = driver.execute_script("return document.documentElement.scrollHeight")
        
        SCROLL_PAUSE_TIME = 3
        MAX_RETRIES = 5
        retries = 0

        while True:
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´
            driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);")
            time.sleep(SCROLL_PAUSE_TIME)
            
            # ì˜ìƒ ë§í¬ ìˆ˜ì§‘
            elements = driver.find_elements(By.CSS_SELECTOR, 'a#video-title-link')
            for elem in elements:
                href = elem.get_attribute("href")
                if href and "watch?v=" in href:
                    # URL ì •ë¦¬ í•¨ìˆ˜ ì ìš©
                    cleaned_url = clean_youtube_url(href)
                    video_urls.add(cleaned_url)

            new_height = driver.execute_script("return document.documentElement.scrollHeight")
            if new_height == last_height:
                retries += 1
                if retries >= MAX_RETRIES:
                    break
            else:
                retries = 0
            last_height = new_height

        video_count = len(video_urls)
        if video_count > 0:
            logger.info(f"âœ… ì´ {video_count}ê°œì˜ ì˜ìƒ URL ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤")

        return list(video_urls)
    except Exception as e:
        logger.error(f"âŒ ì˜ìƒ ID ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return []

# ----------------------------- â¬‡ï¸ elementì˜ text ì¶”ì¶œí•˜ëŠ” ìœ í‹¸ í•¨ìˆ˜ -----------------------------
def safe_get_text(element, default=""):
    try:
        return element.text.strip()
    except Exception:
        return default


# ---------------------- â¬‡ï¸ ì¡°íšŒìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: ì¡°íšŒìˆ˜ 1,234íšŒ -> 1234) ----------------------
def parse_view_count(text: str) -> int:
    try:
        if not text:
            return 0
        # ì¡°íšŒìˆ˜ì™€ íšŒë¥¼ ì œê±°í•˜ê³  ìˆ«ìì™€ ì†Œìˆ˜ì , ë‹¨ìœ„(ë§Œ)ë§Œ ë‚¨ê¹€
        cleaned = text.replace("ì¡°íšŒìˆ˜", "").replace("íšŒ", "").replace(",", "").strip()
        
        # ë°± ë‹¨ìœ„ê°€ ìˆëŠ” ê²½ìš°
        if "ì²œ" in cleaned:
            number = float(cleaned.replace("ë°±", ""))
            return int(number * 1000)
        # ë§Œ ë‹¨ìœ„ê°€ ìˆëŠ” ê²½ìš°
        elif "ë§Œ" in cleaned:
            number = float(cleaned.replace("ë§Œ", ""))
            return int(number * 10000)
        
        return int(cleaned)
    except ValueError as e:
        logger.warning(f"âš ï¸ ì¡°íšŒìˆ˜ íŒŒì‹± ì‹¤íŒ¨: '{text}', ì´ìœ : {e}")
        return 0


# -------------- â¬‡ï¸ êµ¬ë…ì ìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: 1.2ë§Œëª… -> 12000) ------------------
def parse_subscriber_count(text: str) -> int:
    try:
        # êµ¬ë…ìì™€ ëª…ì„ ì œê±°í•˜ê³  ìˆ«ìì™€ ì†Œìˆ˜ì , ë‹¨ìœ„(ì²œ, ë§Œ)ë§Œ ë‚¨ê¹€
        text = text.replace("êµ¬ë…ì", "").replace("ëª…", "").replace(",", "").strip()
        
        if "ì²œ" in text:
            number = float(text.replace("ì²œ", ""))
            return int(number * 1000)
        elif "ë§Œ" in text:
            number = float(text.replace("ë§Œ", ""))
            return int(number * 10000)
        
        return int(text)
    except Exception as e:
        logger.warning(f"âš ï¸ êµ¬ë…ì ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: '{text}', ì´ìœ : {e}")
        return 0
    
    
# ---------------------- â¬‡ï¸ ê°€ê²© í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€ ----------------------
def parse_price(price_text: str) -> int:
    try:
        if not price_text or pd.isna(price_text):
            return 0
        # 'â‚©' ê¸°í˜¸ì™€ ì‰¼í‘œ ì œê±° í›„ ìˆ«ìë§Œ ì¶”ì¶œ
        cleaned_price = re.sub(r'[â‚©,\s]', '', price_text)
        # ìˆ«ìê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë³€í™˜
        if re.search(r'\d', cleaned_price):
            return int(re.sub(r'[^\d]', '', cleaned_price))
        return 0
    except Exception as e:
        logger.warning(f"âš ï¸ ê°€ê²© ë³€í™˜ ì‹¤íŒ¨: {price_text}, ì—ëŸ¬: {e}")
        return 0


# ---------------------- â¬‡ï¸ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ----------------------
def format_date(date_str: str) -> str:
    try:
        if match := re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.?', date_str):
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d}"
        elif match := re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', date_str):
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d}"
        elif match := re.search(r'(\d{4})(\d{2})(\d{2})', date_str):
            year, month, day = match.groups()
            return f"{year}-{month}-{day}"
    except Exception as e:
        logger.warning(f"âš ï¸ ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨: {date_str}, ì—ëŸ¬: {e}")
    return date_str


# ---------------------- â¬‡ï¸ ì„¤ëª…ë€ì˜ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì œê±° ----------------------
def clean_description(text: str) -> str:
    if not text:
        return ""
    # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ í†µì¼
    text = re.sub(r'\n\s*\n', '\n', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    return text.strip()


# ---------------------- â¬‡ï¸ ì œí’ˆ ê°œìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: 5ê°œ ì œí’ˆ) ----------------------
def parse_product_count(text: str) -> Union[int, None]:
    try:
        if match := re.search(r'(\d+)\s*ê°œ\s*ì œí’ˆ', text):
            return int(match.group(1))
    except:
        logger.warning(f"âš ï¸ ì œí’ˆ ê°œìˆ˜ ëª» ì°¾ì•˜ëŠ”ë…??")
    return None

# ---------------------- â¬‡ï¸ ë”ë³´ê¸° í´ë¦­ ë° ë”ë³´ê¸°ë€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ----------------------
def click_description(driver) -> str:
    try:
        # ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¼ìœ¼ë¡œì¨ ë²„íŠ¼ì´ ë¡œë“œë˜ë„ë¡ ìœ ë„
        body = driver.find_element(By.TAG_NAME, 'body')
        for _ in range(3):
            body.send_keys(Keys.END)
            time.sleep(1)
        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„ (2ê°€ì§€ selector)
        try:
            expand_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "#expand"))
            )
            driver.execute_script("arguments[0].click();", expand_button)
            logger.info("ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
        except Exception:
            logger.info("ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ ë˜ëŠ” í´ë¦­ ì‹¤íŒ¨, ë¬´ì‹œí•˜ê³  ì§„í–‰")

        selectors = [
            "#description-text-container",
            "#description-inline-expander"
        ]
        for selector in selectors:
            try:
                elem = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                desc = elem.text.strip()
                if desc:
                    return desc
            except Exception:
                logger.debug(f"'{selector}'ë¡œ ì„¤ëª…ë€ ì¶”ì¶œ ì‹¤íŒ¨, ë‹¤ìŒ ì‹œë„")
        logger.warning("ë”ë³´ê¸°ë€ì— ì„¤ëª…ì´ ì—†ìŒ")
        return "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
        
    except Exception as e:
        logger.error(f"âŒ ì„¤ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)
        return "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
    
    
#--------------------------------------- ì œí’ˆ ì •ë³´ ì¶”ì¶œ -------------------------------------
def extract_products_from_dom(driver) -> list[dict]:
    products = []
    try:
        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„
        try:
            more_button = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "#expand"))
            )
            driver.execute_script("arguments[0].click();", more_button)
            logger.info("âœ… ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
            time.sleep(2)  # ì œí’ˆ ì •ë³´ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        except Exception as e:
            logger.info(f"ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ (ì´ë¯¸ í¼ì³ì ¸ ìˆì„ ìˆ˜ ìˆìŒ): {e}")

        # ì œí’ˆ ì•„ì´í…œ ì°¾ê¸°
        product_items = driver.find_elements(By.CSS_SELECTOR, "#items > ytd-merch-shelf-item-renderer")
        total_items = len(product_items)
        logger.info(f"ì´ {total_items}ê°œì˜ ì œí’ˆ ì•„ì´í…œì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

        '''250526 ì œí’ˆ ì •ë³´ ì¶”ì¶œ ìˆ˜ì •'''
        for item in product_items:
            try:
                product_info = {}

                '''250526 ì œí’ˆëª… ì¶”ì¶œ'''
                title_elem = item.find_element(By.CSS_SELECTOR, ".small-item-hide.product-item-title")
                if title_elem:
                    product_info["title"] = title_elem.text.strip()
                    logger.info(f"âœ… ì œí’ˆëª… ì¶”ì¶œ ì„±ê³µ: {product_info['title']}")
                else:
                    logger.warning("âš ï¸ ì œí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹¤ìŒ ì•„ì´í…œìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤")
                    continue

                '''250526 ì œí’ˆ ë§í¬ ì¶”ì¶œ'''
                link_elem = item.find_element(By.CSS_SELECTOR, "div.product-item-description")
                if link_elem:
                    product_info["url"] = link_elem.text.strip()
                    logger.info(f"âœ… ì œí’ˆ ë§í¬ ì¶”ì¶œ ì„±ê³µ: {product_info['url']}")

                '''250526 ê°€ê²© ì¶”ì¶œ'''
                price_elem = item.find_element(By.CSS_SELECTOR, ".product-item-price")
                if price_elem:
                    product_info["price"] = price_elem.text.strip()
                    logger.info(f"âœ… ì œí’ˆ ê°€ê²© ì¶”ì¶œ ì„±ê³µ: {product_info['price']}")
                else:
                    logger.warning("âš ï¸ ê°€ê²© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹¤ìŒ ì•„ì´í…œìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤")
                    continue

                '''250526 ì´ë¯¸ì§€ URL ì¶”ì¶œ'''
                try:
                    img_elem = item.find_element(By.CSS_SELECTOR, "img.style-scope.yt-img-shadow")
                    img_url = img_elem.get_attribute("src")
                    if img_url and not any(keyword in img_url.lower() for keyword in ['avatar', 'channel', 'profile']):
                        product_info["imageUrl"] = img_url
                        logger.info(f"âœ… ì œí’ˆ ì´ë¯¸ì§€ URL ì¶”ì¶œ ì„±ê³µ: {img_url}")
                    else:
                        product_info["imageUrl"] = ""
                        logger.warning("âš ï¸ ì±„ë„ í”„ë¡œí•„ ì´ë¯¸ì§€ë¡œ íŒë‹¨ë˜ì–´ ì œì™¸ë¨")
                except Exception as e:
                    logger.error(f"âŒ ì´ë¯¸ì§€ URL ì¶”ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
                    product_info["imageUrl"] = ""

                '''250526 íŒë§¤ì²˜ ì¶”ì¶œ'''
                try:
                    merchant_elem = item.find_element(By.CSS_SELECTOR, ".product-item-merchant-text")
                    if merchant_elem:
                        merchant_name = merchant_elem.text.strip().replace("!", "")
                        product_info["merchant"] = merchant_name
                        logger.info(f"âœ… íŒë§¤ì²˜ ì¶”ì¶œ ì„±ê³µ: {merchant_name}")
                except Exception:
                    product_info["merchant"] = ""

                # ì œí’ˆëª…ê³¼ ê°€ê²©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                if "title" in product_info and "price" in product_info:
                    products.append(product_info)
                    logger.info(f"âœ… ì œí’ˆ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {product_info['title']} ({product_info['price']})")

            except Exception as e:
                logger.error(f"âŒ ì œí’ˆ ì •ë³´ ì¶”ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
                continue

        logger.info(f"ì´ {len(products)}ê°œì˜ ì œí’ˆ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
        return products

    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ì œí’ˆ ì¶”ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return []

# ---------------------- â¬‡ï¸ ì˜ìƒ ê¸°ë³¸ ì •ë³´: ì œëª©, ì±„ë„ëª…, êµ¬ë…ì ìˆ˜, ì¡°íšŒìˆ˜, ì—…ë¡œë“œì¼, ì œí’ˆ ê°œìˆ˜ ----------------------
def base_youtube_info(driver, video_url: str) -> pd.DataFrame:
    logger.info("Crawling video: %s", video_url)
    today_str = datetime.today().strftime('%Y%m%d')

    try:
        driver.get(video_url)
        # 250525 í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        time.sleep(5)  # 3ì´ˆì—ì„œ 5ì´ˆë¡œ ì¦ê°€
        
        # í˜ì´ì§€ ìŠ¤í¬ë¡¤ì„ ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰í•˜ì—¬ ë™ì  ì»¨í…ì¸  ë¡œë“œ
        for _ in range(5):  # 3íšŒì—ì„œ 5íšŒë¡œ ì¦ê°€
            driver.execute_script("window.scrollTo(0, window.scrollY + 500);")
            time.sleep(3)  # 2ì´ˆì—ì„œ 3ì´ˆë¡œ ì¦ê°€
        
        wait = WebDriverWait(driver, 20)
        
        # 250523 ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
        expand_button_selectors = [
            "tp-yt-paper-button#expand", "#expand", "#expand-button", "#more",
            "ytd-button-renderer#more", "ytd-expander#description [aria-label='ë”ë³´ê¸°']",
            "ytd-expander[description-collapsed] #expand"
        ]
        
        for selector in expand_button_selectors:
            try:
                more_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector)))
                driver.execute_script("arguments[0].click();", more_button)
                logger.info(f"ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ: {selector}")
                time.sleep(3)  # ë”ë³´ê¸° í´ë¦­ í›„ ì»¨í…ì¸  ë¡œë“œ ëŒ€ê¸°
                break
            except:
                continue
                
        # 250523 ì œí’ˆ ì„¹ì…˜ ì„ íƒì§€ ì¶”ê°€
        product_selectors = [
            "ytd-product-metadata-badge-renderer", "ytd-merch-shelf-renderer",
            "ytd-product-item-renderer","#product-shelf", "#product-list",
            "ytd-merch-product-renderer", "#product-items", ".product-item",
            "#content ytd-metadata-row-container-renderer",
            "ytd-metadata-row-renderer", "#product-section"
        ]
        
        for selector in product_selectors:
            try:
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                logger.info(f"ì œí’ˆ ì„¹ì…˜ ì°¾ìŒ: {selector}")
                break
            except:
                continue
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        soup_file_path = "/Users/mac/Desktop/minmin/intern/crawling_auto_code/soup_files"
        
        # soup_files ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        if not os.path.exists(soup_file_path):
            os.makedirs(soup_file_path)
            
        # í˜„ì¬ ë‚ ì§œë¥¼ YYYYMMDD í˜•ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
        today_str = datetime.now().strftime("%y%m%d")
        
        # ì˜¤ëŠ˜ ë‚ ì§œì˜ íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ê°€ì¥ í° ë²ˆí˜¸ ì°¾ê¸°
        existing_files = [f for f in os.listdir(soup_file_path) if f.endswith(f"_{today_str}.txt")]
        current_number = 1

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        video_id = video_url.split("v=")[-1]
        
        '''250522 ì œëª© (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)'''
        title_selectors = [
            "h1.title yt-formatted-string",
            "h1.title",
            "#title h1",
            "#container h1.style-scope.ytd-watch-metadata"
        ]
        title = None
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                if title:
                    break
        title = title or "ì œëª© ì—†ìŒ"
        logger.info(f"ì œëª©: {title}")

        '''250522 ì±„ë„ëª… (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)'''
        channel_selectors = [
            "ytd-channel-name yt-formatted-string#text a",
            "ytd-channel-name a",
            "#channel-name a"
        ]
        channel_name = None
        for selector in channel_selectors:
            channel_tag = soup.select_one(selector)
            if channel_tag:
                channel_name = channel_tag.text.strip()
                break
        channel_name = channel_name or "ì±„ë„ ì—†ìŒ"

        '''250522 êµ¬ë…ì ìˆ˜'''
        sub_selectors = [
            "yt-formatted-string#owner-sub-count",
            "#subscriber-count"
        ]
        subscriber_count = None
        for selector in sub_selectors:
            sub_tag = soup.select_one(selector)
            if sub_tag:
                subscriber_count = sub_tag.text.strip()
                break
        subscriber_count = subscriber_count or "êµ¬ë…ì ìˆ˜ ì—†ìŒ"

        '''250522 ì¡°íšŒìˆ˜'''
        view_selectors = [
            "span.view-count",
            "#view-count",
            "ytd-video-view-count-renderer"
        ]
        view_count = None
        for selector in view_selectors:
            view_tag = soup.select_one(selector)
            if view_tag:
                view_count = view_tag.text.strip()
                break
        view_count = view_count or "ì¡°íšŒìˆ˜ ì—†ìŒ"

        '''250522 ì—…ë¡œë“œì¼'''
        date_selectors = [
            "#info-strings yt-formatted-string",
            "#upload-info .date",
            "ytd-video-primary-info-renderer yt-formatted-string.ytd-video-primary-info-renderer:not([id])"
        ]
        upload_date = None
        for selector in date_selectors:
            date_tag = soup.select_one(selector)
            if date_tag:
                upload_date = date_tag.text.strip()
                break
        upload_date = upload_date or "ë‚ ì§œ ì—†ìŒ"

        '''250522 ì„¤ëª…ë€'''
        desc_selectors = [
            "ytd-expander#description yt-formatted-string",
            "#description",
            "#description-inline-expander",
            "#description-text-container"
        ]
        description = None
        for selector in desc_selectors:
            desc_tag = soup.select_one(selector)
            if desc_tag:
                description = desc_tag.text.strip()
                if description:
                    break
        description = description or "ì„¤ëª… ì—†ìŒ"
        logger.info(f"ì„¤ëª… ê¸¸ì´: {len(description)} ê¸€ì")

        '''250522 ì œí’ˆ ì¶”ì¶œ'''
        products = extract_products_from_dom(driver)
        if products is None:  # None ì²´í¬ ì¶”ê°€
            products = []
        product_count = len(products)
        logger.info(f"âœ… ì˜ìƒ ì •ë³´ ë° ì œí’ˆ {product_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

        # ê¸°ë³¸ ë°ì´í„° ì„¸íŠ¸
        base_data = []
        
        # 250523 ì œí’ˆì´ ìˆëŠ” ê²½ìš°, ê° ì œí’ˆë³„ë¡œ row ìƒì„±
        if products:
            for product in products:
                row_data = {
                    "youtube_id": video_id,
                    "title": title,
                    "channel_name": channel_name,
                    "subscribers": subscriber_count,
                    "view_count": view_count,
                    "upload_date": upload_date,
                    "extracted_date": today_str,
                    "video_url": video_url,
                    "description": description,
                    "product_count": product_count,
                    "product_name": product.get("title", ""),
                    "product_price": product.get("price", ""),
                    "product_image_url": product.get("imageUrl", ""),
                    "product_merchant_url": product.get("url", ""),
                    "product_merchant": product.get("merchant", "")
                }
                base_data.append(row_data)
        else:
            # ì œí’ˆì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥
            base_data.append({
                "youtube_id": video_id,
                "title": title,
                "channel_name": channel_name,
                "subscribers": subscriber_count,
                "view_count": view_count,
                "upload_date": upload_date,
                "extracted_date": today_str,
                "video_url": video_url,
                "description": description,
                "product_count": 0,
                "product_name": "",
                "product_price": "",
                "product_image_url": "",
                "product_merchant_url": "",
                "product_merchant": ""
            })

        logger.info(f"ğŸ“¦ ìˆ˜ì§‘ëœ ì œí’ˆ ê°œìˆ˜: {len(base_data)}")
        return pd.DataFrame(base_data)
    
    except Exception as e:
        logger.error(f"âŒ base_youtube_info ì˜ˆì™¸: {e}", exc_info=True)
        return pd.DataFrame()

# ----------------------------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì˜ìƒ URL ì ‘ì† í›„ ë°ì´í„° ìˆ˜ì§‘ ìˆ˜í–‰ -----------------------------------------------

def collect_video_data(driver, video_id: str, index: int = None, total: int = None) -> pd.DataFrame:
    # URL ì •ë¦¬
    base_url = clean_youtube_url(f"https://www.youtube.com/watch?v={video_id}")
    
    try:
        driver.get(base_url)
        if index is not None and total is not None:
            logger.info(f"\nğŸ“¹ ({index}/{total}) í¬ë¡¤ë§ ì¤‘: {video_id}")

        df = base_youtube_info(driver, base_url)

        logger.info(f"ğŸ“¦ ìˆ˜ì§‘ëœ ì œí’ˆ ê°œìˆ˜: {len(df)}")
        if df.empty:
            logger.warning(f"âš ï¸ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŒ: {video_id}")

        return df
    
    except Exception as e:
            logger.error(f"âŒ ì˜ˆì™¸ ë°œìƒ - collect_video_data(): {video_id} | ì—ëŸ¬: {e}")
            return None

# ------------------------------------- â¬‡ï¸ í¬ë¡¤ë§ëœ ìœ íŠœë¸Œ ì˜ìƒì„ ì¡°íšŒí•˜ê³  ìˆ˜ì •í•˜ëŠ” ì½”ë“œ ------------------------------
def update_youtube_data_to_db(dataframe: pd.DataFrame) -> int:
    if dataframe.empty:
        return 0

    video_id = dataframe.iloc[0]['video_id']
    
    try:
        video = YouTubeVideo.objects.get(video_id=video_id)
        row = dataframe.iloc[0]

        # ê¸°ì¡´ ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸
        video.extracted_date = row['extracted_date']
        video.upload_date = row['upload_date']
        video.channel_name = row['channel_name']
        video.subscriber_count = row['subscriber_count']
        video.video_url = row['video_url']
        video.title = row['title']
        video.view_count = row['view_count']
        video.product_count = row['product_count']
        video.description = row['description']
        video.save()

        # ê¸°ì¡´ ì œí’ˆ ì •ë³´ ì‚­ì œ í›„ ìƒˆë¡œ ì €ì¥
        video.products.all().delete()

        # pd.DataFrame == dataframe
        for _, row in dataframe.iterrows():
            product_name = row.get('product_name')
            if product_name and pd.notna(product_name):
                product, created = YouTubeProduct.objects.update_or_create(
                    video=video,
                    product_name=row.get('title', 'ì œí’ˆ ì—†ìŒ'),
                    defaults={
                        "product_price": row.get('price'),
                        "product_image_link": row.get('imageUrl'),
                        "product_merchant_link": row.get('url')
                    }
                )
        logger.info(f"ğŸ” ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {video_id}")
        return 1

    except YouTubeVideo.DoesNotExist:
        logger.warning(f"âŒ í•´ë‹¹ video_idì— ëŒ€í•œ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤: {video_id}")
        return 0

# ------------------------------------- â¬‡ï¸ ì±„ë„ URLì—ì„œ ê³ ìœ í•œ ID ì¶”ì¶œ (ì˜ˆ: UCxxxx ë˜ëŠ” @handle í˜•ì‹) ------------------------------
def get_channel_id_from_url(channel_url):
    parsed = urlparse(channel_url)
    parts = parsed.path.strip("/").split("/")
    return parts[-1] if parts else "unknown_channel"

# ------------------------------------- â¬‡ï¸ ì±„ë„ URLì—ì„œ ê³ ìœ í•œ ID ì¶”ì¶œ (ì˜ˆ: UCxxxx ë˜ëŠ” @handle í˜•ì‹) ------------------------------
def get_channel_name(driver, channel_url):
    """
    ì±„ë„ ì´ë¦„ì„ YouTube ì±„ë„ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜´.
    """
    driver.get(channel_url)
    driver.implicitly_wait(5)
    try:
        title_element = driver.find_element("xpath", '//meta[@property="og:title"]')
        channel_name = title_element.get_attribute("content")
        return slugify(channel_name)  # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ slugify ì²˜ë¦¬
    except Exception as e:
        logger.warning(f"âš ï¸ ì±„ë„ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return "unknown_channel"
    
# ------------------------------------- â¬‡ï¸ ì—‘ì…€ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def save_to_excel(df: pd.DataFrame, file_path: str):
    try:
        today_str = datetime.now().strftime("%Y%m%d")
        if file_path.lower().endswith(".xlsx"):
            file_path = file_path[:-5] + f"_{today_str}.xlsx"
        else:
            file_path = file_path + f"_{today_str}.xlsx"

        df.to_excel(file_path, index=False)
        logger.info(f"ğŸ’¾ ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {file_path}")
    except Exception as e:
        logger.error(f"âŒ ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)


# ------------------------------------- â¬‡ï¸ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def save_to_db(data: pd.DataFrame):
    """DataFrameì„ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    if data is None or data.empty:
        logger.warning("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    from django.db import transaction
    saved_count = 0
    updated_count = 0
    
    try:
        with transaction.atomic():
            # ê° ê³ ìœ í•œ video_idì— ëŒ€í•´ í•œ ë²ˆë§Œ ì²˜ë¦¬
            unique_videos = data.drop_duplicates(subset=['youtube_id'])
            
            for _, row in unique_videos.iterrows():
                video_id = row.get("youtube_id")
                if not video_id:
                    logger.warning("âš ï¸ video_id ì—†ìŒ, ê±´ë„ˆëœë‹ˆë‹¤")
                    continue

                # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                extracted_date = format_date(row.get("extracted_date", ""))
                upload_date = format_date(row.get("upload_date", ""))
                
                # êµ¬ë…ì ìˆ˜ì™€ ì¡°íšŒìˆ˜ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
                subscriber_count = parse_subscriber_count(row.get("subscribers", "0"))
                view_count = parse_view_count(row.get("view_count", "0"))
                
                # ì„¤ëª…ë€ ì •ë¦¬
                description = clean_description(row.get("description", ""))

                # ì˜ìƒ ì •ë³´ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
                video_obj, created = YouTubeVideo.objects.update_or_create(
                    video_id=video_id,
                    defaults={
                        "extracted_date": extracted_date,
                        "upload_date": upload_date,
                        "channel_name": row.get("channel_name"),
                        "subscriber_count": subscriber_count,
                        "title": row.get("title"),
                        "view_count": view_count,
                        "video_url": row.get("video_url"),
                        "product_count": row.get("product_count", 0),
                        "description": description,
                    }
                )

                if created:
                    logger.info(f"âœ¨ ìƒˆë¡œìš´ ì˜ìƒ ìƒì„±: {video_id}")
                else:
                    logger.info(f"ğŸ”„ ê¸°ì¡´ ì˜ìƒ ì—…ë°ì´íŠ¸: {video_id}")
                    updated_count += 1

                # í•´ë‹¹ video_idë¥¼ ê°€ì§„ ëª¨ë“  ì œí’ˆ ì •ë³´ ì²˜ë¦¬
                video_products = data[data['youtube_id'] == video_id]
                
                # ê¸°ì¡´ ì œí’ˆ ì •ë³´ ì‚­ì œ
                video_obj.products.all().delete()
                
                # ìƒˆë¡œìš´ ì œí’ˆ ì •ë³´ ì €ì¥
                for _, product_row in video_products.iterrows():
                    product_name = product_row.get("product_name")
                    if product_name and pd.notna(product_name) and product_name.strip():
                        # ê°€ê²©ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                        price = parse_price(product_row.get("product_price", "0"))
                        product, created = YouTubeProduct.objects.update_or_create(
                            video=video_obj,
                            product_name=product_name,
                            defaults={
                                "product_price": price,
                                "product_image_link": product_row.get("product_image_url", ""),
                                "product_merchant_link": product_row.get("product_merchant_url", ""),
                                "product_merchant": product_row.get("product_merchant", "")
                            }
                        )
                        saved_count += 1
                        if created:
                            logger.info(f"âœ¨ ìƒˆë¡œìš´ ì œí’ˆ ì •ë³´ ì €ì¥: {product_name} (ê°€ê²©: {price:,}ì›)")
                        else:
                            logger.info(f"ğŸ”„ ê¸°ì¡´ ì œí’ˆ ì •ë³´ ì—…ë°ì´íŠ¸: {product_name} (ê°€ê²©: {price:,}ì›)")

    except Exception as e:
        logger.error(f"âŒ DB ì €ì¥ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}", exc_info=True)
        return 0

    logger.info(f"âœ… ì´ {updated_count}ê°œì˜ ì˜ìƒì´ ì—…ë°ì´íŠ¸ë˜ì—ˆê³ , {saved_count}ê°œì˜ ìƒˆë¡œìš´ ì œí’ˆì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return saved_count

# ------------------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì±„ë„ì˜ ì „ì²´ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def crawl_channel_videos(channel_url: str, save_path: str):
    with create_driver() as driver:
        video_ids = get_all_video_ids(driver, channel_url)

        total = len(video_ids)
        if total == 0:
            logger.warning("âŒ ì±„ë„ì—ì„œ ìˆ˜ì§‘ëœ ì˜ìƒ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ì´ {total}ê°œ ì˜ìƒ í¬ë¡¤ë§ ì‹œì‘")
        all_data = pd.DataFrame()

        for i, video_id in enumerate(video_ids, start=1):
            try:
                logger.info(f"\nğŸ” ({i}/{total}) ì˜ìƒ í¬ë¡¤ë§ ì‹œì‘: {video_id}")
                df = collect_video_data(driver, video_id)
                if df is not None and not df.empty:
                    save_to_db(df)
                    logger.info(f"âœ… ({i}/{total}) ì˜ìƒ í¬ë¡¤ë§ ì™„ë£Œ: {video_id}")
            except Exception as e:
                logger.error(f"âŒ ({i}/{total}) ì˜ìƒ í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ ë°œìƒ: {video_id}, ì—ëŸ¬: {e}", exc_info=True)

        if not all_data.empty:
            save_to_db(all_data)
            save_to_excel(all_data, save_path)
        else:
            logger.warning("âš ï¸ í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„° ì—†ìŒ")


# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":

    logging.basicConfig(level=logging.INFO)

    channel_urls = [
        "https://www.youtube.com/@%EC%B9%A1%EC%B4%89",
    ]
    
    export_dir = "exports"
    if not os.path.exists(export_dir):
        os.makedirs(export_dir)

    for channel_url in channel_urls:
        try:
            logger.info(f"ğŸš€ ì±„ë„ í¬ë¡¤ë§ ì‹œì‘: {channel_url}")

            today_str = datetime.datetime.now().strftime("%Y%m%d")
            channel_name = urllib.parse.unquote(channel_url.split("/")[-1])
            save_path = os.path.join(export_dir,f"{channel_name}_{today_str}.xlsx")

            crawl_channel_videos(channel_url, save_path)
            logger.info(f"âœ… ì±„ë„ í¬ë¡¤ë§ ì™„ë£Œ: {channel_url}")

        except Exception as e:
            logger.warning(f"âŒ ì±„ë„ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {channel_url} - {e}")
        
        time.sleep(1)  # ê° ì±„ë„ ê°„ 1ì´ˆ ì‰¬ì—ˆë‹¤ê°€ ë‹¤ìŒ ì±„ë„ ì‹¤í–‰