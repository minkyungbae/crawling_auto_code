from .models import YouTubeVideo, YouTubeProduct
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Tuple, Union
import pandas as pd
import time
import json
import re


# ----------------------------- íŒŒì‹± í•¨ìˆ˜(ì—…ë¡œë“œì¼ ë‚ ì§œ í‘œê¸°, ì¡°íšŒìˆ˜, ì œí’ˆ ìˆ˜) -----------------------------

def parse_subscriber_count(text: str) -> str:
    """êµ¬ë…ì ìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: 1.2ë§Œëª… -> 12000)"""
    match = re.search(r'([\d\.]+)([ì²œë§Œ]?)ëª…', text)
    if not match:
        return "0"

    number = float(match.group(1))
    unit = match.group(2)

    if unit == 'ì²œ':
        number *= 1_000
    elif unit == 'ë§Œ':
        number *= 10_000

    return f"{int(number):,}"


def parse_upload_date(text: str) -> Union[str, None]:
    """ì—…ë¡œë“œ ë‚ ì§œ ë¬¸ìì—´ì„ 'YYYYMMDD' í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if m := re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.', text):
        year, month, day = m.groups()
        return f"{year}{int(month):02d}{int(day):02d}"
    elif m := re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', text):
        year, month, day = m.groups()
        return f"{year}{int(month):02d}{int(day):02d}"
    return None


def parse_view_count(text: str) -> Union[int, None]:
    """ì¡°íšŒìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: ì¡°íšŒìˆ˜ 1,234íšŒ -> 1234)"""
    if match := re.search(r'ì¡°íšŒìˆ˜\s*([\d,]+)íšŒ', text):
        return int(match.group(1).replace(',', ''))
    return None


def parse_product_count(text: str) -> Union[int, None]:
    """ì œí’ˆ ê°œìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: 5ê°œ ì œí’ˆ)"""
    if match := re.search(r'(\d+)\s*ê°œ\s*ì œí’ˆ', text):
        return int(match.group(1))
    return None


def extract_video_info(info_texts: List[str]) -> Tuple[Union[int, None], str, Union[int, None]]:
    """
    ì˜ìƒ ì •ë³´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¡°íšŒìˆ˜, ì—…ë¡œë“œì¼, ì œí’ˆ ê°œìˆ˜ íŒŒì‹±
    """
    view_count = None
    upload_date = "ì—…ë¡œë“œ ë‚ ì§œ ì •ë³´ ëª» ì°¾ìŒ"
    product_count = None

    for raw in info_texts:
        soup = BeautifulSoup(raw, 'html.parser')
        text = soup.get_text(separator=' ').strip()

        view_count = view_count or parse_view_count(text)
        upload_date = parse_upload_date(text) or upload_date
        product_count = product_count or parse_product_count(text)

    return view_count, upload_date, product_count


# ---------------------- ì˜ìƒ ê¸°ë³¸ ì •ë³´: ì œëª©, ì±„ë„ëª…, êµ¬ë…ì ìˆ˜ ----------------------

def extract_basic_video_info(driver) -> Tuple[str, str, str]:
    """ì˜ìƒ ê¸°ë³¸ ì •ë³´: ì œëª©, ì±„ë„ëª…, êµ¬ë…ì ìˆ˜ ìˆ˜ì§‘"""
    try:
        title = WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "h1.style-scope.ytd-watch-metadata yt-formatted-string"))
        ).text.strip()
    except Exception:
        title = "ì œëª© ìˆ˜ì§‘ ì‹¤íŒ¨"

    try:
        channel_name = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "yt-formatted-string#text a.yt-simple-endpoint"))
        ).text.strip()
    except Exception:
        channel_name = "ì±„ë„ëª… ìˆ˜ì§‘ ì‹¤íŒ¨"

    try:
        subscriber_text = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#owner-sub-count"))
        ).text.strip()
        subscriber_count = parse_subscriber_count(subscriber_text)
    except Exception:
        subscriber_count = "êµ¬ë…ì ìˆ˜ ìˆ˜ì§‘ ì‹¤íŒ¨"

    return title, channel_name, subscriber_count


# ---------------------- ë”ë³´ê¸° í´ë¦­ ë° ë”ë³´ê¸°ë€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ----------------------
def click_show_more_and_get_description(driver) -> str:
    """
    ì˜ìƒ ì„¤ëª…ë€ 'ë”ë³´ê¸°' í´ë¦­ í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    try:
        body = driver.find_element(By.TAG_NAME, 'body')
        for _ in range(3):  # ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì„œ 'ë”ë³´ê¸°' ë²„íŠ¼ ë‚˜ì˜¤ê²Œ ìœ ë„
            body.send_keys(Keys.END)
            time.sleep(1)

        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.ID, "description-inline-expander"))
        )

        expand_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "tp-yt-paper-button#expand"))
        )
        driver.execute_script("arguments[0].click();", expand_button)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "description-inline-expander")))

    except Exception:
        pass

    try:
        description = driver.find_element(By.ID, "description-inline-expander").text
        if not description.strip():
            description = "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
    except Exception:
        description = "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"

    return description

#--------------------------------------- ì œí’ˆ ì •ë³´ë¥¼ jsonìœ¼ë¡œ ì°¾ì•„ì„œ ê°–ê³  ì˜¤ê¸° -------------------------------------
def extract_products_from_json(driver) -> list:
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    html_text = soup.prettify()

    def extract_json_block(text: str, key: str) -> str:
        """
        JSON ë°°ì—´ ë¸”ë¡ì„ ì¤‘ê´„í˜¸/ëŒ€ê´„í˜¸ ê· í˜•ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        """
        pattern = rf'"{key}":\s*(\[[\s\S]*?\])'
        match = re.search(pattern, text)
        if not match:
            return None

        start = match.start(1)
        stack = []
        for i in range(start, len(text)):
            if text[i] == '[':
                stack.append('[')
            elif text[i] == ']':
                stack.pop()
                if not stack:
                    return text[start:i+1]
        return None

    products_json_text = extract_json_block(html_text, "productsData")
    if not products_json_text:
        print("âŒ productsData ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return []

    try:
        products = json.loads(products_json_text)
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        with open("error_products.json", "w", encoding="utf-8") as f:
            f.write(products_json_text)
        return []

    extracted = []
    for i, item in enumerate(products):
        renderer = item.get("productListItemRenderer", {})

        # â¬‡ï¸ ì œí’ˆëª…
        product_name = renderer.get("title", {}).get("simpleText")
        if product_name:
            product_name = product_name.lstrip() # ë§¨ ì• ê³µë°±ë§Œ ì œê±°


        # â¬‡ï¸ ê°€ê²©
        price_info = renderer.get("price")
        price = price_info.get("simpleText") if isinstance(price_info, dict) else price_info
        if price:
            price = price.replace(",", "").replace("â‚©", "").strip()


        # â¬‡ï¸ íŒë§¤ì²˜ ì°¾ê¸°
        commands = renderer.get("onClickCommand", {}) \
            .get("commandExecutorCommand", {}) \
            .get("commands", [])

        merchant_link = None
        for cmd in commands:
            url = cmd.get("commandMetadata", {}).get("webCommandMetadata", {}).get("url")
            if url:
                merchant_link = url
                break

        # â¬‡ï¸ HTMLì—ì„œ íŒë§¤ì²˜ ë§í¬ë¥¼ ë³´ì™„ ì¶”ì¶œ
        if not merchant_link:
            descriptions = soup.select("div.product-item-description")
            if i < len(descriptions):
                merchant_text = descriptions[i].get_text(strip=True)
                if merchant_text:
                    # http ì—†ëŠ” ë§í¬ ì²˜ë¦¬
                    merchant_link = (
                        "https://" + merchant_text if not merchant_text.startswith("http") else merchant_text
                    )
        
        # â¬‡ï¸ ì œí’ˆ ì‚¬ì§„ ì°¾ê¸°
        thumbnails = renderer.get("thumbnail", {}).get("thumbnails", [])

        # 256px ì œí’ˆ ì‚¬ì§„ì´ ì—†ì„ ê²½ìš° ì²« ë²ˆì§¸ ì´ë¯¸ì§€ fallback
        image_url = None
        for thumb in thumbnails:
            if thumb.get("width") == 256:
                image_url = thumb.get("url")
                break
        if not image_url and thumbnails:
            image_url = thumbnails[0].get("url")

        print(f"âœ… ìƒí’ˆ {i+1}: {product_name}, ê°€ê²©: {price}, íŒë§¤ì²˜: {merchant_link}, ì´ë¯¸ì§€: {image_url}")
        extracted.append({
            "product_name": product_name,
            "product_price": price,
            "product_link": merchant_link,
            "product_image_link": image_url,
        })

    return extracted

#--------------------------------------- â¬‡ï¸ product ì¶”ì¶œ -------------------------------------
def extract_products_and_metadata(driver, video_id: str, title: str, channel_name: str, subscriber_count: str, description: str) -> pd.DataFrame:
    """
    ì˜ìƒ í˜ì´ì§€ ì „ì²´ HTMLì„ íŒŒì‹±í•˜ì—¬ ì œí’ˆ ì •ë³´ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í›„ DataFrame ë°˜í™˜
    """
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    info_texts = [span.get_text(strip=True) for span in soup.select("span.style-scope.yt-formatted-string.bold") if span.get_text(strip=True)]
    
    # ----------------- ë””ë²„ê¹… í™•ì¸í•˜ê¸° ------------------------------------
    # with open("youtube_product_html.txt", "w", encoding="utf-8") as f:
    #     f.write(soup.prettify())
    # print("HTML êµ¬ì¡°ê°€ 'youtube_product_html.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    view_count, upload_date, product_count = extract_video_info(info_texts)
    today_str = datetime.today().strftime('%Y%m%d')
    base_url = f"https://www.youtube.com/watch?v={video_id}"

    # í˜ì´ì§€ ë Œë”ë§ ëŒ€ê¸°
    time.sleep(5)

    product_data = {
        "video_id": video_id,
        "title": title,
        "channel_name": channel_name,
        "subscriber_count": subscriber_count,
        "view_count": view_count,
        "upload_date": upload_date,
        "extracted_date": today_str,
        "video_url": base_url,
        "product_count": product_count,
        "description": description,
    }

    #------------------------------------------ â¬‡ï¸ ë””ë²„ê¹… --------------------------------------------------
    # # img íƒœê·¸ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë“  ì´ë¯¸ì§€ ì¶”ì¶œ
    # with open("youtube_product_html.txt", "r", encoding="utf-8") as f:
    #     soup = BeautifulSoup(f, "html.parser")

    # # ì´ë¯¸ì§€ íƒœê·¸ ëª¨ë‘ íƒìƒ‰
    # img_tags = soup.find_all("img")
    # print(f"ì „ì²´ img íƒœê·¸ ìˆ˜: {len(img_tags)}")

    # for i, img in enumerate(img_tags):
    #     if img.has_attr("src"):
    #         print(f"{i+1}. ì´ë¯¸ì§€ URL: {img['src']}")
    #     elif img.has_attr("style"):
    #         match = re.search(r'background-image:\s*url\("([^"]+)"\)', img["style"])
    #         if match:
    #             print(f"{i+1}. ì´ë¯¸ì§€ styleì—ì„œ URL: {match.group(1)}")

    # # ì œí’ˆ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ” ìš”ì†Œ ì¶”ì¶œ
    # candidates = soup.find_all(True, class_=re.compile(r"(product|merch|shop)", re.IGNORECASE))
    # print(f"ê´€ë ¨ëœ ìš”ì†Œ ìˆ˜: {len(candidates)}")
    # for i, tag in enumerate(candidates[:10]):
    #     print(f"{i+1}. íƒœê·¸ ì´ë¦„: {tag.name}, í´ë˜ìŠ¤: {tag.get('class')}")
    #     print(tag.prettify()[:500])

    #----------------------------------------- ë””ë²„ê¹… ë -------------------------------------------------
    
    product_info_list = []

    extracted_products = extract_products_from_json(driver)

    for product in extracted_products:
        product_info_list.append({**product_data, **product})

    if not product_info_list:
        product_info_list.append({**product_data,
            "product_image_link": None,
            "product_name": None,
            "product_price": None,
            "product_link": None})
        

    return pd.DataFrame(product_info_list)


# ----------------------------------------------- â¬‡ï¸ ë©”ì¸ ì§„ì… í•¨ìˆ˜ -----------------------------------------------

def collect_video_data(driver, video_id: str, index: int = None, total: int = None) -> pd.DataFrame:
    """
    ìœ íŠœë¸Œ ì˜ìƒ URL ì ‘ì† í›„ ë°ì´í„° ìˆ˜ì§‘ ìˆ˜í–‰
    """
    base_url = f"https://www.youtube.com/watch?v={video_id}"
    driver.get(base_url)

    if index is not None and total is not None:
        print(f"\nğŸ“¹ ({index}/{total}) í¬ë¡¤ë§ ì¤‘: {video_id}")

    title, channel_name, subscriber_count = extract_basic_video_info(driver)
    print(f"  - ì œëª©: {title} | ì±„ë„: {channel_name} | êµ¬ë…ì: {subscriber_count}")

    description = click_show_more_and_get_description(driver)
    df = extract_products_and_metadata(driver, video_id, title, channel_name, subscriber_count, description)

    return df


# -------------------------------------------- â¬‡ï¸ DB ì €ì¥ í•¨ìˆ˜ ----------------------------------------

def save_youtube_data_to_db(dataframe: pd.DataFrame) -> int:
    """
    ìˆ˜ì§‘í•œ DataFrame ë°ì´í„°ë¥¼ Django DBì— ì €ì¥
    """
    if dataframe.empty:
        return 0

    video_id = dataframe.iloc[0]['video_id']

    if YouTubeVideo.objects.filter(video_id=video_id).exists():
        print(f"âš ï¸ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì˜ìƒ idì…ë‹ˆë‹¤: {video_id}")
        return 0

    row = dataframe.iloc[0]
    video = YouTubeVideo.objects.create(
        video_id=video_id,
        extracted_date=row['extracted_date'],
        upload_date=row['upload_date'],
        channel_name=row['channel_name'],
        subscriber_count=row['subscriber_count'],
        video_url=row['video_url'],
        title=row['title'],
        view_count=row['view_count'],
        product_count=row['product_count'],
        description=row['description']
    )

    for _, row in dataframe.iterrows():
        product_name = row.get('product_name')
        if product_name and pd.notna(product_name):
            YouTubeProduct.objects.create(
                video=video,
                product_image_link=row.get('product_image_link'),
                product_name=product_name,
                product_price=row.get('product_price'),
                product_link=row.get('product_link'),
            )
    print(f"âœ… ì˜ìƒ ë° ì œí’ˆ ì •ë³´ ì €ì¥ ì™„ë£Œ: {video_id}")
    return 1
