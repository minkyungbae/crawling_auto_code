from .models import YouTubeVideo, YouTubeProduct
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Tuple, Union, Dict
import pandas as pd
import logging
import time
import json
import re

logger = logging.getLogger(__name__)  # ëª¨ë“ˆ ì „ìš© ë¡œê±° ìƒì„±

def crawl_youtube():
    logger.info("ìœ íŠœë¸Œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    logger.warning("í…ŒìŠ¤íŠ¸ ê²½ê³  ë©”ì‹œì§€ì…ë‹ˆë‹¤.")
    logger.info("ìœ íŠœë¸Œ í¬ë¡¤ë§ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")



# ----------------------------- â¬‡ï¸ íŒŒì‹± í•¨ìˆ˜(ì—…ë¡œë“œì¼ ë‚ ì§œ í‘œê¸°, ì¡°íšŒìˆ˜, ì œí’ˆ ìˆ˜) -----------------------------

def parse_subscriber_count(text: str) -> str:
    """êµ¬ë…ì ìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: 1.2ë§Œëª… -> 12000)"""
    match = re.search(r'([\d\.]+)([ì²œë§Œ]?)ëª…', text)
    if not match:
        return "0"

    number = float(match.group(1))
    unit = match.group(2)

    if unit == 'ì²œ':
        number *= 1_000
    elif unit == 'ë§Œ':
        number *= 10_000

    return f"{int(number):,}"

# ---------------------- â¬‡ï¸ ì—…ë¡œë“œ ë‚ ì§œ ë¬¸ìì—´ì„ 'YYYYMMDD' í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ----------------------
def parse_upload_date(text: str) -> Union[str, None]:

    if m := re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.', text):
        year, month, day = m.groups()
        return f"{year}{int(month):02d}{int(day):02d}"
    elif m := re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', text):
        year, month, day = m.groups()
        return f"{year}{int(month):02d}{int(day):02d}"
    return None

# ---------------------- â¬‡ï¸ ì¡°íšŒìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: ì¡°íšŒìˆ˜ 1,234íšŒ -> 1234) ----------------------
def parse_view_count(text: str) -> Union[int, None]:

    if match := re.search(r'ì¡°íšŒìˆ˜\s*([\d,]+)íšŒ', text):
        return int(match.group(1).replace(',', ''))
    return None


# ---------------------- â¬‡ï¸ ì œí’ˆ ê°œìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: 5ê°œ ì œí’ˆ) ----------------------
def parse_product_count(text: str) -> Union[int, None]:

    if match := re.search(r'(\d+)\s*ê°œ\s*ì œí’ˆ', text):
        return int(match.group(1))
    return None

# ---------------------- â¬‡ï¸ ì¡°íšŒìˆ˜, ì—…ë¡œë“œì¼, ì œí’ˆ ê°œìˆ˜ íŒŒì‹± ----------------------
def extract_video_info(info_texts: List[str]) -> Tuple[Union[int, None], str, Union[int, None]]:

    view_count = None
    upload_date = "ì—…ë¡œë“œ ë‚ ì§œ ì •ë³´ ëª» ì°¾ìŒ"
    product_count = None

    for raw in info_texts:
        soup = BeautifulSoup(raw, 'html.parser')
        text = soup.get_text(separator=' ').strip()

        view_count = view_count or parse_view_count(text)
        upload_date = parse_upload_date(text) or upload_date
        product_count = product_count or parse_product_count(text)

    return view_count, upload_date, product_count


# ---------------------- â¬‡ï¸ ì˜ìƒ ê¸°ë³¸ ì •ë³´: ì œëª©, ì±„ë„ëª…, êµ¬ë…ì ìˆ˜ ----------------------

def extract_basic_video_info(driver) -> Tuple[str, str, str]:

    try:
        title = WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "h1.style-scope.ytd-watch-metadata yt-formatted-string"))
        ).text.strip()
    except Exception:
        title = "ì œëª© ìˆ˜ì§‘ ì‹¤íŒ¨"

    try:
        channel_name = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "yt-formatted-string#text a.yt-simple-endpoint"))
        ).text.strip()
    except Exception:
        channel_name = "ì±„ë„ëª… ìˆ˜ì§‘ ì‹¤íŒ¨"

    try:
        subscriber_text = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#owner-sub-count"))
        ).text.strip()
        subscriber_count = parse_subscriber_count(subscriber_text)
    except Exception:
        subscriber_count = "êµ¬ë…ì ìˆ˜ ìˆ˜ì§‘ ì‹¤íŒ¨"

    return title, channel_name, subscriber_count


# ---------------------- â¬‡ï¸ ë”ë³´ê¸° í´ë¦­ ë° ë”ë³´ê¸°ë€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ----------------------
def click_show_more_and_get_description(driver) -> str:

    try:
        body = driver.find_element(By.TAG_NAME, 'body')
        for _ in range(3):  # ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì„œ 'ë”ë³´ê¸°' ë²„íŠ¼ ë‚˜ì˜¤ê²Œ ìœ ë„
            body.send_keys(Keys.END)
            time.sleep(1)

        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.ID, "description-inline-expander"))
        )

        expand_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "tp-yt-paper-button#expand"))
        )
        driver.execute_script("arguments[0].click();", expand_button)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "description-inline-expander")))

    except Exception as e:
        logging.error(f"âŒ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")

    try:
        description = driver.find_element(By.ID, "description-inline-expander").text
        if not description.strip():
            description = "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
    except Exception:
        description = "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"

    return description

#--------------------------------------- ì œí’ˆ ì •ë³´ë¥¼ jsonìœ¼ë¡œ ì°¾ì•„ì„œ ê°–ê³  ì˜¤ê¸° -------------------------------------
def extract_products_from_json(driver) -> list:
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    html_text = soup.prettify()

    def extract_json_block(text: str, key: str) -> str:
        """
        JSON ë°°ì—´ ë¸”ë¡ì„ ì¤‘ê´„í˜¸/ëŒ€ê´„í˜¸ ê· í˜•ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        """
        pattern = rf'"{key}":\s*(\[[\s\S]*?\])'
        match = re.search(pattern, text)
        if not match:
            return None

        start = match.start(1)
        stack = []
        for i in range(start, len(text)):
            if text[i] == '[':
                stack.append('[')
            elif text[i] == ']':
                stack.pop()
                if not stack:
                    return text[start:i+1]
        return None

    products_json_text = extract_json_block(html_text, "productsData")
    if not products_json_text:
        logging.warning("âš ï¸ productsData ë¸”ë¡ ì—†ìŒ: ì´ ì˜ìƒì—ëŠ” ì œí’ˆ ì •ë³´ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return []

    try:
        products = json.loads(products_json_text)
    except json.JSONDecodeError as e:
        logging.error(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        with open("error_products.json", "w", encoding="utf-8") as f:
            f.write(products_json_text)
        return []

    extracted = []

    for i, item in enumerate(products):
        renderer = item.get("productListItemRenderer", {})

        # â¬‡ï¸ ì œí’ˆëª…
        product_name = renderer.get("title", {}).get("simpleText")
        if product_name:
            product_name = product_name.lstrip() # ë§¨ ì• ê³µë°±ë§Œ ì œê±°


        # â¬‡ï¸ ê°€ê²©
        price_info = renderer.get("price")
        price = price_info.get("simpleText") if isinstance(price_info, dict) else price_info
        if price:
            price = price.replace(",", "").replace("â‚©", "").strip()


        # â¬‡ï¸ íŒë§¤ì²˜ ì°¾ê¸°
        commands = renderer.get("onClickCommand", {}) \
            .get("commandExecutorCommand", {}) \
            .get("commands", [])

        merchant_link = None
        for cmd in commands:
            url = cmd.get("commandMetadata", {}).get("webCommandMetadata", {}).get("url")
            if url:
                merchant_link = url
                break

        # â¬‡ï¸ HTMLì—ì„œ íŒë§¤ì²˜ ë§í¬ë¥¼ ë³´ì™„ ì¶”ì¶œ
        if not merchant_link:
            descriptions = soup.select("div.product-item-description")
            if i < len(descriptions):
                merchant_text = descriptions[i].get_text(strip=True)
                if merchant_text:
                    # http ì—†ëŠ” ë§í¬ ì²˜ë¦¬
                    merchant_link = (
                        "https://" + merchant_text if not merchant_text.startswith("http") else merchant_text
                    )
        
        # â¬‡ï¸ ì œí’ˆ ì‚¬ì§„ ì°¾ê¸°
        thumbnails = renderer.get("thumbnail", {}).get("thumbnails", [])

        # 256px ì œí’ˆ ì‚¬ì§„ì´ ì—†ì„ ê²½ìš° ì²« ë²ˆì§¸ ì´ë¯¸ì§€ fallback
        image_url = None
        for thumb in thumbnails:
            if thumb.get("width") == 256:
                image_url = thumb.get("url")
                break
        if not image_url and thumbnails:
            image_url = thumbnails[0].get("url")

        logging.info(f"âœ… ìƒí’ˆ {i+1}: {product_name}, ê°€ê²©: {price}, íŒë§¤ì²˜: {merchant_link}, ì´ë¯¸ì§€: {image_url}")
        extracted.append({
            "product_name": product_name,
            "product_price": price,
            "product_link": merchant_link,
            "product_image_link": image_url,
        })

    return extracted

#--------------------------------------- â¬‡ï¸ product ì¶”ì¶œ -------------------------------------
def extract_products_and_metadata(driver, video_id: str, title: str, channel_name: str, subscriber_count: str, description: str) -> List[Dict]:
    """
    ì˜ìƒ í˜ì´ì§€ ì „ì²´ HTMLì„ íŒŒì‹±í•˜ì—¬ ì œí’ˆ ì •ë³´ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í›„ DataFrame ë°˜í™˜
    """
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    info_texts = [span.get_text(strip=True) for span in soup.select("span.style-scope.yt-formatted-string.bold") if span.get_text(strip=True)]
    
    # ----------------------------ë””ë²„ê¹… í™•ì¸í•˜ê¸° ------------------------------------
    # with open("youtube_product_html.txt", "w", encoding="utf-8") as f:
    #     f.write(soup.prettify())
    # print("HTML êµ¬ì¡°ê°€ 'youtube_product_html.txt' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # ----------------------------ë””ë²„ê¹… í™•ì¸í•˜ê¸° ë ------------------------------------

    view_count, upload_date, product_count = extract_video_info(info_texts)
    today_str = datetime.today().strftime('%Y%m%d') # ì˜¤ëŠ˜ ë‚ ì§œ(YYYYmmdd)
    base_url = f"https://www.youtube.com/watch?v={video_id}" # ê¸°ë³¸ ìœ íŠœë¸Œ ì˜ìƒ í‹€

    # í˜ì´ì§€ ë Œë”ë§ ëŒ€ê¸°
    time.sleep(5)

    product_count = product_count or 0
    product_data = {
        "video_id": video_id,
        "title": title,
        "channel_name": channel_name,
        "subscriber_count": subscriber_count,
        "view_count": view_count,
        "upload_date": upload_date,
        "extracted_date": today_str,
        "video_url": base_url,
        "product_count": product_count,
        "description": description,
    }
    try:
        product_info_list = []
        extracted_products = extract_products_from_json(driver)

        for product in extracted_products:
            product_info_list.append({**product_data, **product})

        if not product_info_list:
            product_info_list.append({
                **product_data,
                "product_image_link": None,
                "product_name": "í•´ë‹¹ ì˜ìƒì— í¬í•¨ëœ ì œí’ˆ ì—†ìŒ",
                "product_price": None,
                "product_link": None
                })
        return pd.DataFrame(product_info_list)
    
    except Exception as e:
        logging.error(f"[âŒ ì˜ˆì™¸ ë°œìƒ] {video_id} ì²˜ë¦¬ ì¤‘: {e}")

        return [{
            **product_data,
            "product_image_link": None,
            "product_name": "í•´ë‹¹ ì˜ìƒì— í¬í•¨ëœ ì œí’ˆ ì—†ìŒ",
            "product_price": None,
            "product_link": None
        }]
    
    # return pd.DataFrame(product_info_list)


# ----------------------------------------------- â¬‡ï¸ ë©”ì¸ ì§„ì… í•¨ìˆ˜ -----------------------------------------------

def collect_video_data(driver, video_id: str, index: int = None, total: int = None) -> pd.DataFrame:
    """
    ìœ íŠœë¸Œ ì˜ìƒ URL ì ‘ì† í›„ ë°ì´í„° ìˆ˜ì§‘ ìˆ˜í–‰
    """
    base_url = f"https://www.youtube.com/watch?v={video_id}"
    driver.get(base_url)

    if index is not None and total is not None:
        logging.info(f"\nğŸ“¹ ({index}/{total}) í¬ë¡¤ë§ ì¤‘: {video_id}")

    title, channel_name, subscriber_count = extract_basic_video_info(driver)
    logging.info(f"  - ì œëª©: {title} | ì±„ë„: {channel_name} | êµ¬ë…ì: {subscriber_count}")

    description = click_show_more_and_get_description(driver)
    df = extract_products_and_metadata(driver, video_id, title, channel_name, subscriber_count, description)

    return df


# -------------------------------------------- â¬‡ï¸ DB(sqlite) ì €ì¥ í•¨ìˆ˜ ----------------------------------------

def save_youtube_data_to_db(dataframe: pd.DataFrame) -> int:

    if dataframe.empty:
        return 0

    video_id = dataframe.iloc[0]['video_id']

    if YouTubeVideo.objects.filter(video_id=video_id).exists():
        logging.warning(f"âš ï¸ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì˜ìƒ idì…ë‹ˆë‹¤: {video_id}")
        return 0

    row = dataframe.iloc[0]
    video = YouTubeVideo.objects.create(
        video_id=video_id,
        extracted_date=row['extracted_date'],
        upload_date=row['upload_date'],
        channel_name=row['channel_name'],
        subscriber_count=row['subscriber_count'],
        video_url=row['video_url'],
        title=row['title'],
        view_count=row['view_count'],
        product_count=row['product_count'],
        description=row['description']
    )

    # ë¹ˆ ê°’ì¼ ë•Œ
    for _, row in dataframe.iterrows():
        product_name = row.get('product_name')

        # ë¹ˆ ê°’ ì²˜ë¦¬
        if not product_name or pd.isna(product_name) or product_name.strip() == "":
            product_name = "ì˜ìƒì— í¬í•¨ëœ ì œí’ˆ ì •ë³´ ì—†ìŒ"
        
        product_image_link = row.get('product_image_link')
        if not product_image_link or pd.isna(product_image_link) or product_image_link.strip() == "":
            product_image_link = "ì˜ìƒì— í¬í•¨ëœ ì œí’ˆ ì •ë³´ ì—†ìŒ"
        
        product_price = row.get('product_price')
        if not product_price or pd.isna(product_price) or product_price.strip() == "":
            product_price = "ì˜ìƒì— í¬í•¨ëœ ì œí’ˆ ì •ë³´ ì—†ìŒ"
        
        product_link = row.get('product_link')
        if not product_link or pd.isna(product_link) or product_link.strip() == "":
            product_link = None
            
        YouTubeProduct.objects.create(
            video=video,
            product_image_link=row.get('product_image_link'),
            product_name=product_name,
            product_price=row.get('product_price'),
            product_link=row.get('product_link'),
        )
    logging.info(f"âœ… ì˜ìƒ ë° ì œí’ˆ ì •ë³´ ì €ì¥ ì™„ë£Œ: {video_id}")
    return 1

# ------------------------------------- â¬‡ï¸ í¬ë¡¤ë§ëœ ìœ íŠœë¸Œ ì˜ìƒì„ ì¡°íšŒí•˜ê³  ìˆ˜ì •í•˜ëŠ” ì½”ë“œ ------------------------------
def update_youtube_data_to_db(dataframe: pd.DataFrame) -> int:
    if dataframe.empty:
        return 0

    video_id = dataframe.iloc[0]['video_id']
    
    try:
        video = YouTubeVideo.objects.get(video_id=video_id)
        row = dataframe.iloc[0]

        # ê¸°ì¡´ ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸
        video.extracted_date = row['extracted_date']
        video.upload_date = row['upload_date']
        video.channel_name = row['channel_name']
        video.subscriber_count = row['subscriber_count']
        video.video_url = row['video_url']
        video.title = row['title']
        video.view_count = row['view_count']
        video.product_count = row['product_count']
        video.description = row['description']
        video.save()

        # ê¸°ì¡´ ì œí’ˆ ì •ë³´ ì‚­ì œ í›„ ìƒˆë¡œ ì €ì¥
        video.products.all().delete()

        # pd.DataFrame == dataframe
        for _, row in dataframe.iterrows():
            product_name = row.get('product_name')
            if product_name and pd.notna(product_name):
                YouTubeProduct.objects.create(
                    video=video,
                    product_image_link=row.get('product_image_link'),
                    product_name=product_name,
                    product_price=row.get('product_price'),
                    product_link=row.get('product_link'),
                )
        logging.info(f"ğŸ” ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {video_id}")
        return 1

    except YouTubeVideo.DoesNotExist:
        logging.error(f"âŒ í•´ë‹¹ video_idì— ëŒ€í•œ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤: {video_id}")
        return 0

