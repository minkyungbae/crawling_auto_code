# --------- í”„ë¡œì íŠ¸ì—ì„œ importí•œ ëª©ë¡ ---------------
from .models import YouTubeVideo, YouTubeProduct
from config import settings
# --------- seleniumì—ì„œ importí•œ ëª©ë¡ ---------------
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
# --------- webdriverì—ì„œ importí•œ ëª©ë¡ ---------------
from webdriver_manager.chrome import ChromeDriverManager
from contextlib import contextmanager # ë“œë¼ì´ë²„ ê´€ë¦¬í•˜ëŠ” íƒœê·¸
# --------- ê·¸ ì™¸ í¬ë¡¤ë§ ì½”ë“œë¥¼ ìœ„í•´ importí•œ ëª©ë¡ ---------------
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Union, Dict, Optional
from urllib.parse import urlparse, unquote
from slugify import slugify
import pandas as pd
import logging, time, re, json, os, urllib.parse


# ----------------------------- â¬‡ï¸ logging ì„¤ì • -----------------------------

logger = logging.getLogger(__name__)  # logger.info(), logger.warning()ë§Œ ì¨ì•¼í•´ìš©

# --------- driver í•œ ë²ˆìœ¼ë¡œ ì •ì˜ ---------------
@contextmanager
def create_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-infobars")
    options.add_argument("--start-maximized")
    options.add_argument("--disable-notifications")
    options.add_argument('--ignore-certificate-errors')
    options.add_argument('--ignore-ssl-errors')
    # User-Agent ì„¤ì •
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36')
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    logger.info("ğŸŸ¢ ChromeDriver ì‹¤í–‰")
    try:
        yield driver
    except Exception as e:
        logger.error(f"âŒ WebDriver ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
        raise
    finally:
        driver.quit()
        logger.info("ğŸ›‘ ChromeDriver ì¢…ë£Œ")


# ---------------------- â¬‡ï¸ URL ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€ ----------------------
def clean_youtube_url(url: str) -> str:
    """YouTube URLì„ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # URLì—ì„œ 'watch?v=' ë¶€ë¶„ì´ ì¤‘ë³µë˜ëŠ”ì§€ í™•ì¸
        if url.count('watch?v=') > 1:
            # ë§ˆì§€ë§‰ 'watch?v=' ì´í›„ì˜ ë¶€ë¶„ë§Œ ê°€ì ¸ì˜´
            video_id = url.split('watch?v=')[-1]
            return f'https://www.youtube.com/watch?v={video_id}'
        return url
    except Exception as e:
        logger.error(f"âŒ URL ì •ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return url


# ----------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì±„ë„ì˜ ì˜ìƒ ì „ë¶€ ê°€ì§€ê³  ì˜¤ëŠ” í•¨ìˆ˜ -----------------------------
def get_all_video_ids(driver, channel_url):
    logger.info(f"ğŸ” ì±„ë„ ì˜ìƒ ID ìˆ˜ì§‘ ì‹œì‘: {channel_url}")

    try:
        videos_url = channel_url.rstrip('/') + "/videos"
        driver.get(videos_url)
        time.sleep(5)  # í˜ì´ì§€ ë¡œë”©ì„ ìœ„í•œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€

        video_urls = set()
        last_height = driver.execute_script("return document.documentElement.scrollHeight")
        
        SCROLL_PAUSE_TIME = 3
        MAX_RETRIES = 5
        retries = 0

        while True:
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´
            driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);")
            time.sleep(SCROLL_PAUSE_TIME)
            
            # ì˜ìƒ ë§í¬ ìˆ˜ì§‘
            elements = driver.find_elements(By.CSS_SELECTOR, 'a#video-title-link')
            for elem in elements:
                href = elem.get_attribute("href")
                if href and "watch?v=" in href:
                    # URL ì •ë¦¬ í•¨ìˆ˜ ì ìš©
                    cleaned_url = clean_youtube_url(href)
                    video_urls.add(cleaned_url)

            new_height = driver.execute_script("return document.documentElement.scrollHeight")
            if new_height == last_height:
                retries += 1
                if retries >= MAX_RETRIES:
                    break
            else:
                retries = 0
            last_height = new_height

        video_count = len(video_urls)
        if video_count > 0:
            logger.info(f"âœ… ì´ {video_count}ê°œì˜ ì˜ìƒ URL ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤")

        return list(video_urls)
    except Exception as e:
        logger.error(f"âŒ ì˜ìƒ ID ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return []

# ----------------------------- â¬‡ï¸ elementì˜ text ì¶”ì¶œí•˜ëŠ” ìœ í‹¸ í•¨ìˆ˜ -----------------------------
def safe_get_text(element, default=""):
    try:
        return element.text.strip()
    except Exception:
        return default


# ---------------------- â¬‡ï¸ ì¡°íšŒìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: ì¡°íšŒìˆ˜ 1,234íšŒ -> 1234) ----------------------
def parse_view_count(text: str) -> int:
    try:
        if not text:
            return 0
        cleaned = text.replace("ì¡°íšŒìˆ˜", "").replace("íšŒ", "").replace(",", "").strip()
        return int(cleaned)
    except ValueError as e:
        logger.warning(f"âš ï¸ ì¡°íšŒìˆ˜ íŒŒì‹± ì‹¤íŒ¨: '{text}', ì´ìœ : {e}")
        return 0


# ----------------------------- â¬‡ï¸ êµ¬ë…ì ìˆ˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì í˜•íƒœë¡œ ë³€í™˜ (ì˜ˆ: 1.2ë§Œëª… -> 12000) -----------------------------
def parse_subscriber_count(text: str) -> int:
    try:
        text = text.replace("ëª…", "").replace(",", "").strip()
        if "ì²œ" in text:
            return int(float(text.replace("ì²œ", "")) * 1_000)
        elif "ë§Œ" in text:
            return int(float(text.replace("ë§Œ", "")) * 10_000)
        return int(text)
    except Exception as e:
        logger.warning(f"âš ï¸ êµ¬ë…ì ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: '{text}', ì´ìœ : {e}")
        return 0


# ---------------------- â¬‡ï¸ ì—…ë¡œë“œ ë‚ ì§œ ë¬¸ìì—´ì„ 'YYYYMMDD' í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ----------------------
def parse_upload_date(text: str) -> Union[str, None]:
    try:
        if match := re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.', text):
            year, month, day = match.groups()
            return f"{year}{int(month):02d}{int(day):02d}"
        elif match := re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', text):
            year, month, day = match.groups()
            return f"{year}{int(month):02d}{int(day):02d}"
    except:
        logger.warning(f"âš ï¸ ë‚ ì§œë¥¼ í˜•ì‹ ë³€í™˜ ëª»í–ˆëŠ”ë…??")
    return None


# ---------------------- â¬‡ï¸ ì œí’ˆ ê°œìˆ˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: 5ê°œ ì œí’ˆ) ----------------------
def parse_product_count(text: str) -> Union[int, None]:
    try:
        if match := re.search(r'(\d+)\s*ê°œ\s*ì œí’ˆ', text):
            return int(match.group(1))
    except:
        logger.warning(f"âš ï¸ ì œí’ˆ ê°œìˆ˜ ëª» ì°¾ì•˜ëŠ”ë…??")
    return None

# ---------------------- â¬‡ï¸ ë”ë³´ê¸° í´ë¦­ ë° ë”ë³´ê¸°ë€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ----------------------
def click_description(driver) -> str:
    try:
        # ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¼ìœ¼ë¡œì¨ ë²„íŠ¼ì´ ë¡œë“œë˜ë„ë¡ ìœ ë„
        body = driver.find_element(By.TAG_NAME, 'body')
        for _ in range(3):
            body.send_keys(Keys.END)
            time.sleep(1)
        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„ (2ê°€ì§€ selector)
        try:
            expand_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "#expand"))
            )
            driver.execute_script("arguments[0].click();", expand_button)
            logger.info("ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
        except Exception:
            logger.info("ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ ë˜ëŠ” í´ë¦­ ì‹¤íŒ¨, ë¬´ì‹œí•˜ê³  ì§„í–‰")

        selectors = [
            "#description-text-container",
            "#description-inline-expander"
        ]
        for selector in selectors:
            try:
                elem = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                desc = elem.text.strip()
                if desc:
                    return desc
            except Exception:
                logger.debug(f"'{selector}'ë¡œ ì„¤ëª…ë€ ì¶”ì¶œ ì‹¤íŒ¨, ë‹¤ìŒ ì‹œë„")
        logger.warning("ë”ë³´ê¸°ë€ì— ì„¤ëª…ì´ ì—†ìŒ")
        return "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
        
    except Exception as e:
        logger.error(f"âŒ ì„¤ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)
        return "ë”ë³´ê¸°ë€ì— ì„¤ëª… ì—†ìŒ"
    
    
#--------------------------------------- ì œí’ˆ ì •ë³´ ì¶”ì¶œ -------------------------------------
def extract_products_from_dom(soup: BeautifulSoup) -> list[dict]:
    products = []
    try:
        script_tags = soup.find_all("script")
        for tag in script_tags:
            if "var productsData" in tag.text:
                json_text = tag.string.split("var productsData = ")[1].split(";</script>")[0]
                product_data = json.loads(json_text)
                for product in product_data:
                    products.append({
                        "title": product.get("title", "ì—†ìŒ"),
                        "url": product.get("url", "ì—†ìŒ"),
                        "price": product.get("price", "ì—†ìŒ"),
                        "imageUrl": product.get("imageUrl", "ì—†ìŒ"),
                    })
                break
    except Exception as e:
        logger.error(f"âŒ ì œí’ˆ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return products
    

# ---------------------- â¬‡ï¸ ì˜ìƒ ê¸°ë³¸ ì •ë³´: ì œëª©, ì±„ë„ëª…, êµ¬ë…ì ìˆ˜, ì¡°íšŒìˆ˜, ì—…ë¡œë“œì¼, ì œí’ˆ ê°œìˆ˜ ----------------------
def base_youtube_info(driver, video_url: str) -> pd.DataFrame:
    logger.info("Crawling video: %s", video_url)
    today_str = datetime.today().strftime('%Y%m%d')

    try:
        driver.get(video_url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "description")))
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        video_id = video_url.split("v=")[-1]
        # ì œëª©
        title_tag = soup.select_one("h1.title")
        title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

        # ì±„ë„ëª…
        channel_tag = soup.select_one("ytd-channel-name a")
        channel_name = channel_tag.text.strip() if channel_tag else "ì±„ë„ ì—†ìŒ"

        # êµ¬ë…ì ìˆ˜
        sub_tag = soup.select_one("yt-formatted-string#owner-sub-count")
        subscriber_count = sub_tag.text.strip() if sub_tag else "êµ¬ë…ì ìˆ˜ ì—†ìŒ"

        # ì¡°íšŒìˆ˜
        view_count_tag = soup.select_one("span.view-count")
        view_count = view_count_tag.text.strip() if view_count_tag else "ì¡°íšŒìˆ˜ ì—†ìŒ"

        # ì—…ë¡œë“œì¼
        upload_date_tag = soup.select_one("div#info-strings yt-formatted-string")
        upload_date = upload_date_tag.text.strip() if upload_date_tag else "ë‚ ì§œ ì—†ìŒ"

        # ì„¤ëª…ë€
        desc_tag = soup.select_one("yt-formatted-string.content")
        description = desc_tag.text.strip() if desc_tag else "ì„¤ëª… ì—†ìŒ"

        # ì œí’ˆ ì¶”ì¶œ
        products = extract_products_from_dom(soup)
        product_count = len(products)

        # ê¸°ë³¸ ë°ì´í„° ì„¸íŠ¸
        base_data = {
            "youtube_id": video_id,
            "title": title,
            "channel_name": channel_name,
            "subscribers": subscriber_count,
            "view_count": view_count,
            "upload_date": upload_date,
            "extracted_date": today_str,
            "video_url": video_url,
            "description": description,
            "product_count": product_count,
            "products": [],
        }

        if products:
            for p in products:
                base_data["products"].append({
                    "name": p.get("title", ""),
                    "link": p.get("url", ""),
                    "price": p.get("price", None),
                    "image": p.get("imageUrl", None)
                })
        else:
            base_data["products"] = [{
                "product_name": "ì œí’ˆ ì—†ìŒ",
                "product_link": None,
                "product_price": None,
                "product_image_link": None
            }]

            
        logger.info(f"âœ… ì˜ìƒ ì •ë³´ ë° ì œí’ˆ {product_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return pd.DataFrame([base_data])
    
    except Exception as e:
        logger.error(f"âŒ base_youtube_info ì˜ˆì™¸: {e}", exc_info=True)
        return pd.DataFrame()

# ----------------------------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì˜ìƒ URL ì ‘ì† í›„ ë°ì´í„° ìˆ˜ì§‘ ìˆ˜í–‰ -----------------------------------------------

def collect_video_data(driver, video_id: str, index: int = None, total: int = None) -> pd.DataFrame:
    # URL ì •ë¦¬
    base_url = clean_youtube_url(f"https://www.youtube.com/watch?v={video_id}")
    
    try:
        driver.get(base_url)
        if index is not None and total is not None:
            logger.info(f"\nğŸ“¹ ({index}/{total}) í¬ë¡¤ë§ ì¤‘: {video_id}")

        df = base_youtube_info(driver, base_url)

        logger.info(f"ğŸ“¦ ìˆ˜ì§‘ëœ ì œí’ˆ ê°œìˆ˜: {len(df)}")
        if df.empty:
            logger.warning(f"âš ï¸ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŒ: {video_id}")

        return df
    
    except Exception as e:
            logger.error(f"âŒ ì˜ˆì™¸ ë°œìƒ - collect_video_data(): {video_id} | ì—ëŸ¬: {e}")
            return None

# ------------------------------------- â¬‡ï¸ í¬ë¡¤ë§ëœ ìœ íŠœë¸Œ ì˜ìƒì„ ì¡°íšŒí•˜ê³  ìˆ˜ì •í•˜ëŠ” ì½”ë“œ ------------------------------
def update_youtube_data_to_db(dataframe: pd.DataFrame) -> int:
    if dataframe.empty:
        return 0

    video_id = dataframe.iloc[0]['video_id']
    
    try:
        video = YouTubeVideo.objects.get(video_id=video_id)
        row = dataframe.iloc[0]

        # ê¸°ì¡´ ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸
        video.extracted_date = row['extracted_date']
        video.upload_date = row['upload_date']
        video.channel_name = row['channel_name']
        video.subscriber_count = row['subscriber_count']
        video.video_url = row['video_url']
        video.title = row['title']
        video.view_count = row['view_count']
        video.product_count = row['product_count']
        video.description = row['description']
        video.save()

        # ê¸°ì¡´ ì œí’ˆ ì •ë³´ ì‚­ì œ í›„ ìƒˆë¡œ ì €ì¥
        video.products.all().delete()

        # pd.DataFrame == dataframe
        for _, row in dataframe.iterrows():
            product_name = row.get('product_name')
            if product_name and pd.notna(product_name):
                YouTubeProduct.objects.create(
                    video=video,
                    product_image_link=row.get('product_image_link'),
                    product_name=product_name,
                    product_price=row.get('product_price'),
                    product_link=row.get('product_link'),
                )
        logger.info(f"ğŸ” ì˜ìƒ ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {video_id}")
        return 1

    except YouTubeVideo.DoesNotExist:
        logger.warning(f"âŒ í•´ë‹¹ video_idì— ëŒ€í•œ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤: {video_id}")
        return 0

# ------------------------------------- â¬‡ï¸ ì±„ë„ URLì—ì„œ ê³ ìœ í•œ ID ì¶”ì¶œ (ì˜ˆ: UCxxxx ë˜ëŠ” @handle í˜•ì‹) ------------------------------
def get_channel_id_from_url(channel_url):
    parsed = urlparse(channel_url)
    parts = parsed.path.strip("/").split("/")
    return parts[-1] if parts else "unknown_channel"

# ------------------------------------- â¬‡ï¸ ì±„ë„ URLì—ì„œ ê³ ìœ í•œ ID ì¶”ì¶œ (ì˜ˆ: UCxxxx ë˜ëŠ” @handle í˜•ì‹) ------------------------------
def get_channel_name(driver, channel_url):
    """
    ì±„ë„ ì´ë¦„ì„ YouTube ì±„ë„ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜´.
    """
    driver.get(channel_url)
    driver.implicitly_wait(5)
    try:
        title_element = driver.find_element("xpath", '//meta[@property="og:title"]')
        channel_name = title_element.get_attribute("content")
        return slugify(channel_name)  # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ slugify ì²˜ë¦¬
    except Exception as e:
        logger.warning(f"âš ï¸ ì±„ë„ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return "unknown_channel"
    
# ------------------------------------- â¬‡ï¸ ì—‘ì…€ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def save_to_excel(df: pd.DataFrame, file_path: str):
    try:
        today_str = datetime.now().strftime("%Y%m%d")
        if file_path.lower().endswith(".xlsx"):
            file_path = file_path[:-5] + f"_{today_str}.xlsx"
        else:
            file_path = file_path + f"_{today_str}.xlsx"

        df.to_excel(file_path, index=False)
        logger.info(f"ğŸ’¾ ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {file_path}")
    except Exception as e:
        logger.error(f"âŒ ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)

# ------------------------------------- â¬‡ï¸ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def save_to_db(data: dict):
    from django.db import transaction

    if data.empty:
        logger.warning("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    failed_ids = []

    with transaction.atomic():
        try:
            # DataFrameì˜ ì²« ë²ˆì§¸ í–‰ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            data_dict = data.iloc[0].to_dict()
            video_id = data_dict.get("youtube_id")
            if not video_id:
                logger.warning("âš ï¸ video_id ì—†ìŒ, ì €ì¥ ë¶ˆê°€")
                return
            
            video_data = {
                "extracted_date": data_dict.get("extracted_date"),
                "upload_date": data_dict.get("upload_date"),
                "channel_name": data_dict.get("channel_name"),
                "subscriber_count": data_dict.get("subscribers"),
                "title": data_dict.get("title"),
                "view_count": data_dict.get("view_count"),
                "video_url": data_dict.get("video_url"),
                "product_count": data_dict.get("product_count", 0),
                "description": data_dict.get("description"),
            }

            video_obj = YouTubeVideo.objects.filter(video_id=video_id).first()

            if video_obj:
                # ë³€ê²½ëœ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                has_changes = any(
                    getattr(video_obj, field) != value
                    for field, value in video_data.items()
                )
                if has_changes:
                    for field, value in video_data.items():
                        setattr(video_obj, field, value)
                    video_obj.save()
                    logger.info(f"DB ì—…ë°ì´íŠ¸ ì™„ë£Œ: {video_id}")
                else:
                    logger.info(f"ë³€ê²½ ì—†ìŒ: {video_id}")
            else:
                video_obj = YouTubeVideo.objects.create(video_id=video_id, **video_data)
                logger.info(f"DB ì €ì¥ ì™„ë£Œ: {video_id}")

            # ì œí’ˆ ì €ì¥
            products = data_dict.get("products", [])
            for p in products:
                YouTubeProduct.objects.update_or_create(
                    video=video_obj,
                    product_name=p.get("product_name", "ì œí’ˆ ì—†ìŒ"),
                    defaults={
                        "product_price": p.get("product_price", None),
                        "product_image_link": p.get("product_image_link", None),
                        "product_link": p.get("product_link", None),
                    }
                )

        except Exception as e:
            logger.error(f"âŒ ì €ì¥ ì‹¤íŒ¨ - video_id: {video_id} | ì—ëŸ¬: {e}")
            failed_ids.append(video_id)
        
    # ì‹¤íŒ¨í•œ video_id íŒŒì¼ë¡œ ì €ì¥
    if failed_ids:
        with open("logs/failed_ids.txt", "w") as f:
            for vid in failed_ids:
                f.write(f"{vid}\n")
        logger.warning(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨í•œ video_id {len(failed_ids)}ê°œ ì €ì¥ ì™„ë£Œ: failed_ids.txt")


# ------------------------------------- â¬‡ï¸ ìœ íŠœë¸Œ ì±„ë„ì˜ ì „ì²´ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ ------------------------------
def crawl_channel_videos(channel_url: str, save_path: str):
    with create_driver() as driver:
        video_ids = get_all_video_ids(driver, channel_url)

        total = len(video_ids)
        if total == 0:
            logger.warning("âŒ ì±„ë„ì—ì„œ ìˆ˜ì§‘ëœ ì˜ìƒ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ì´ {total}ê°œ ì˜ìƒ í¬ë¡¤ë§ ì‹œì‘")
        all_data = pd.DataFrame()

        for i, video_id in enumerate(video_ids, start=1):
            try:
                df = collect_video_data(driver, video_id, i, total)
                if df is not None and not df.empty:
                    save_to_db(df)
            except Exception as e:
                logger.error(f"âŒ ì˜ìƒ í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ ë°œìƒ: {video_id}, ì—ëŸ¬: {e}", exc_info=True)

        if not all_data.empty:
            save_to_db(all_data)
            save_to_excel(all_data, save_path)
        else:
            logger.warning("âš ï¸ í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„° ì—†ìŒ")


# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":

    logging.basicConfig(level=logging.INFO)

    channel_urls = [
        "https://www.youtube.com/@yegulyegul8256",
        "https://www.youtube.com/@%EC%B9%A1%EC%B4%89",
    ]
    
    export_dir = "exports"
    if not os.path.exists(export_dir):
        os.makedirs(export_dir)

    for channel_url in channel_urls:
        try:
            logger.info(f"ğŸš€ ì±„ë„ í¬ë¡¤ë§ ì‹œì‘: {channel_url}")

            today_str = datetime.datetime.now().strftime("%Y%m%d")
            channel_name = urllib.parse.unquote(channel_url.split("/")[-1])
            save_path = os.path.join(export_dir,f"{channel_name}_{today_str}.xlsx")

            crawl_channel_videos(channel_url, save_path)
            logger.info(f"âœ… ì±„ë„ í¬ë¡¤ë§ ì™„ë£Œ: {channel_url}")

        except Exception as e:
            logger.warning(f"âŒ ì±„ë„ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {channel_url} - {e}")
        
        time.sleep(3)  # ğŸ”½ ê° ì±„ë„ ê°„ 3ì´ˆ ì‰¬ì—ˆë‹¤ê°€ ë‹¤ìŒ ì±„ë„ ì‹¤í–‰
